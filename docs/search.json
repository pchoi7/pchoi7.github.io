[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/TimnitGebru/index.html",
    "href": "posts/TimnitGebru/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "About Dr. Timnit Gebru\nDr. Timnit Gebru is a computer scientist who specializes machine learning and artificial intelligence. She cofounded a research organization alongside Rediet Abebe known as Black in AI, which seeks to speak out about the lack of diversity in the field of artificial intelligence. Many of her works address racial discrimination and gender bias. Dr. Gebru coauthored a paper wher research showed that facial recognition was less accurate in identifying people of color and women. This would mean that there would be discrimination against these groups. This led to another research by other computer scientists and found that in facial recognition, black women were 35% less likely to be recognized than white men, further proving Dr. Gebru’s argument. Dr. Gebru is a recognized voice in artificial intelligence because she continues to uncover the ethical issues of the field even if it negatively impacts her. For example, she and five others did an research and put their findings on a paper called “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” during her time at Google. Despite the opposition from the higer-ups, Gebru insisted that the paper be published, which led to her departure from the company. Regardless of these things, she continues to move forward with the effort the address social injustices in the field of AI through research and starting organizations like Black in AI and the Distributed Artificial Intelligence Research Institute.\n\n\nTutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision\nIn Dr. Gebru’s talk, she speaks about how the field of computer vision is negatively affecting certain groups of people. She stated that computer vision is being utilized for things like the hiring process, surveillance, and policing. Although computer vision has its pros, there are also cons. For example, Dr. Gebru points out the use of face recognition in Maryland, where they used it to identify the Annapolis Capital Gazette shooter, and while this was a benefit, the police started to use it on photos to identify individuals at the Freddit Gray protests. In addition to surveillance, there are racial issues of computer vision. She noted that there were high disparity between black women and white men. Facial recognition was accurate when identifying white men but were much less accurate when identifying black women. Some companies believed that they did not have enough data to correctly identify certain groups of people such as people of the transgender community. As a result, they used videos of Transgender Youtubers without permission to enhance their data for facial/gender recognition.\nComputer vision algorithms can be useful, but it can cause a lot more harm than one might think.\n\nQuestion for Dr. Gebru\nWhat can companies do to prevent algorithmic biases in computer vision?\n\n\n\nSummary of Dr. Gebru’s Talk\nThe main focus on Dr. Gebru’s talk on Monday was about AGI (artificial general intelligence). In her talk, she briefly goes over the first wave of eugenics, movement that sought to improve the human population through sterilization, selective breeding, etc. and also the second wave of eugenics. Dr. Gebru argues that the current growth of AGI is correlated to the second wave of eugenics. She mentions that both the first and the second wave of eugenics (mostly the latter) are issues that are starting to surface as the field of AGI grows because these AGI models select the traits of humans that it deems helpful to the human population.\nDr. Gebru also explains that AGI is being used to create a utopian society. She describes that basic human needs will be met and the problems of the world such as poverty will be solved through the development in AGI. However, she argues that this utopian society that is promised through AGI will only benefit the rich. Another reason she links the development in AGI to second wave eugenics is due to its transhumanism ideology. Dr. Gebru notes that the field of AGI is being geared towards developing a society where humans and technology are integrated. Transhumanists believe that the integration of human and technology will improve society as a whole. Dr. Gebru raises concerns for this ideology and that only the privileged will be able to improve their living standards while others are left behind.\nI agree with the arguments of Dr. Gebru. Although I am fairly new to the ideas that Dr. Gebru has brought up in the talk, I believe I can agree with the fact that TESCREAL is ethically wrong even if they believe they are doing good for the future of the human population. AI may not seem like it is related to the second wave eugenics, but I can agree with Dr. Gebru with the fact that in the near future, their relation will become clearer.\n\n\nMy Thoughts\nAttending the talk by Dr. Gebru was a great experience and the topic was very interesting. Before attending the talk, I had never heard any of the TESCREAL terms, but after attending, it helped me aware of what is going on in the field of artificial intelligence and that it is being pushed in this direction. I became interested and researched on my own time about how the second wave of eugenics is related to AGI. Something that I was sort of aware was transhumanism; the integration of man and machine is something that I have always disagreed with and after the talk I became certain of my opinion."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying MiddCourses Reviews\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nPenguins blog post\n\n\n\n\n\n\nMay 21, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression\n\n\n\n\n\nBlog Post for Logistic Regression\n\n\n\n\n\n\nMay 13, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nTimnit Gebru\n\n\n\n\n\nBlog Post about Dr. Gebru’s Talk\n\n\n\n\n\n\nApr 20, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nPerceptron\n\n\n\n\n\nPerceptron Blog Post\n\n\n\n\n\n\nMar 10, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blogg"
  },
  {
    "objectID": "posts/Perceptron/perceptronNB.html",
    "href": "posts/Perceptron/perceptronNB.html",
    "title": "Perceptron",
    "section": "",
    "text": "Here is the link to the source code: https://github.com/pchoi7/pchoi7.github.io/blob/main/posts/Perceptron/perceptron.py\n\nfrom perceptron import Perceptron\n\n\nPerceptron Implementation\nIn order to perform the perceptron, I created a fit function. In this fit function, I initiate a feature matrix \\(\\tilde{X}\\) of 1’s. The weight vector is given a random number and a for loop is made to perform the perceptron update. In this for loop, the score is appended into the history and the weight is updated using the equation below:\n\\(\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_{i}\\langle \\tilde{w}^{(t)},\\tilde{x}_{i}\\rangle &lt; 0)\\tilde{y}_{i}\\tilde{x}_{i}\\)\n\n\nExperiment (Linearly Separable)\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nprint(\"The accuracy of the perceptron is \" + str(p.score(X, y)))\nprint(\"Score evolution over time is \" + str(p.history[-10:]))\n\nThe accuracy of the perceptron is 1.0\nScore evolution over time is [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\n\n\nExperiment (Non-linearly Separable)\nFor this part of the experiment, the graph was set up so that it would be impossible to be linearly separable. We can see that the data cannot be separated linearly because some of the purple points overlap the yellow data. The accuracy for this data set will never reach 100%.\n\nnp.random.seed(12345)\n\np_2 = Perceptron()\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-0.2, -0.2), (1.5, 1.5)])\n\nfig_2 = plt.scatter(X_2[:,0], X_2[:,1], c = y)\nfig_2 = draw_line(p.w, -2, 2)\n\nxlab_2 = plt.xlabel(\"Feature 1\")\nylab_2 = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig_2 = plt.plot(p_2.history)\nxlab_2 = plt.xlabel(\"Iteration\")\nylab_2 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np_2 = Perceptron()\np_2.fit(X_2, y_2, max_steps = 1000)\n\nprint(\"The accuracy of the perceptron is \" + str(p_2.score(X_2, y_2)))\nprint(\"Score evolution over time is \" + str(p_2.history[-10:]))\n\nThe accuracy of the perceptron is 0.88\nScore evolution over time is [0.84, 0.84, 0.56, 0.71, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88]\n\n\nWe can see that with this data set, the accuracy may fall well below 50-60% for a few iterations. Overall, the perceptron algorithm is pretty accurate (88%).\n\n\nExperiment (Multi-dimensional)\n\nnp.random.seed(12345)\n\np_3 = Perceptron()\n\np_features = 10\nX_3, y_3 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.8, -1.8), (1.8, 1.8)])\n\np_3.fit(X_3, y_3, max_steps = 1000)\n\nfig_3 = plt.plot(p_3.history)\nxlab_3 = plt.xlabel(\"Iteration\")\nylab_3 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nprint(\"The accuracy of the perceptron is \" + str(p_3.score(X_3, y_3)))\nprint(\"Score evolution over time is \" + str(p_3.history[-10:]))\n\nThe accuracy of the perceptron is 1.0\nScore evolution over time is [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nThis data set of 9 dimensions is linearly separable because it reaches 1.0 accuracy.\n\n\nRuntime Complexity\nFor a single iteration of the perceptron algorithm update, the runtime is O(p). The operations that affect the equation \\(\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_{i}\\langle \\tilde{w}^{(t)},\\tilde{x}_{i}\\rangle &lt; 0)\\tilde{y}_{i}\\tilde{x}_{i}\\) is the dot product, multiplication, then addition. Performing the dot product is O(p) + O(p), while performing multiplication and addition is O(1) for both. Therefore, the runtime for equation 1 is O(p)."
  },
  {
    "objectID": "posts/Logistic-Regression/LR.html",
    "href": "posts/Logistic-Regression/LR.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "URL to source code: code\n\nLogistic Regression Implementation\nIn order to implement the \\(\\textit{fit}\\) function, I used the equation for the gradient as follows:\n\\(\\nabla L(w) = \\frac{1}{n} \\sum_{i = 1}^{n}\\nabla \\ell (f_{w}(x_{i}), y_{i})\\)\nThis equation is the gradient for empirical risk for logistic regression. Using this equation, we can substitute it into \\(w^{(t +1)} \\leftarrow w^{(t)} - \\alpha \\nabla L(w^{(t)})\\) and choose our value of \\(\\alpha\\) until it converges.\nFor stochastic gradient descent, a similar equation to the \\(\\textit{fit}\\) function was used:\n\\(\\nabla_{S}L(w) = \\frac{1}{|S|} \\sum_{i \\in S}\\nabla \\ell (f_{w}(x_{i}), y_{i})\\)\nwhere \\(S \\subseteq [n] = \\{1,...,n\\}\\) and is called the batch size.\n\nfrom logReg import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nEvolution of Loss Function for Gradient Descent and Stochastic Gradient Descent\n\n# GRADIENT\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\n\n# STOCHASTIC GRADIENT\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\n\nExperiment 1 (Large Learning rate \\(\\alpha\\))\nIn this experiment, I changed the values of the learning rate \\(\\alpha\\) to see which values fails to converge.\n\nalpha_list = [0.1, 10, 100]  # list of learning rates to test\n\nfor i in range(len(alpha_list)):\n    \n    if i == 0:\n        \n        LR = LogisticRegression()\n        LR.fit(X, y, alpha = alpha_list[i], max_epochs = 75)\n    \n        loss = len(LR.loss_history)\n        plt.plot(np.arange(loss) + 1, LR.loss_history, label = r\"$\\alpha$ = 0.1\")\n    \n    else:\n           \n        LR = LogisticRegression()\n        LR.fit(X, y, alpha = alpha_list[i], max_epochs = 75)\n    \n        loss = len(LR.loss_history)\n        plt.plot(np.arange(loss) + 1, LR.loss_history, label = r\"$\\alpha$ = %d\"%alpha_list[i])\n    \n\nplt.legend()\nplt.xlabel('Number of iterations')\nplt.ylabel('Loss')\nplt.show()\n\n\n\n\nThe loss of a learning rate too high will not converge. In this case, I’ve used a learning rate of \\(\\alpha = 100\\), and the graph shows that for this value the loss does not converge.\n\n\nExperiment 2 (Changing Batch Size)\nIn this experiment, I am changing the batch size to see how it influences the convergence of the algorithm. This experiment uses the fit_stochastic function.\n\n#LR = LogisticRegression()\n\nbatch_list = [10, 50, 100, 1000]  # list of batch sizes to test\n\nfor i in range(len(batch_list)):\n    \n    LR = LogisticRegression()\n    LR.fit_stochastic(X, y, alpha = 0.1, batch_size = batch_list[i], max_epochs = 100)\n    \n    loss = len(LR.loss_history)\n    plt.plot(np.arange(loss) + 1, LR.loss_history, label = \"batch size = %d\"%batch_list[i])\n    \n    \nplt.legend()\nplt.xlabel('Number of iterations')\nplt.ylabel('Loss')\nplt.show()\n\n\n\n\nAs shown in this graph, when the batch size is smaller, the algorithm converges faster."
  },
  {
    "objectID": "posts/Penguins/penguins.html",
    "href": "posts/Penguins/penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer penguins data used in this blog post was collected by Dr. Kristen Gorman and the Palmer Station, which is a member of Long Term Ecological Research Network. The data contains physical measurements of three penguin species: Adelie, Chinstrap, and Gentoo.\n\nData Preparation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import matplotlib as plt\nfrom matplotlib.patches import Patch\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n27\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN46A1\nYes\n11/29/07\n44.5\n14.3\n216.0\n4100.0\nNaN\n7.96621\n-25.69327\nNaN\n\n\n1\nPAL0708\n22\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN41A2\nYes\n11/27/07\n45.1\n14.5\n215.0\n5000.0\nFEMALE\n7.63220\n-25.46569\nNaN\n\n\n2\nPAL0910\n124\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN67A2\nYes\n11/16/09\n41.4\n18.5\n202.0\n3875.0\nMALE\n9.59462\n-25.42621\nNaN\n\n\n3\nPAL0910\n146\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN82A2\nYes\n11/16/09\n39.0\n18.7\n185.0\n3650.0\nMALE\n9.22033\n-26.03442\nNaN\n\n\n4\nPAL0708\n24\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A2\nNo\n11/28/07\n50.6\n19.4\n193.0\n3800.0\nMALE\n9.28153\n-24.97134\nNaN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n1\n45.1\n14.5\n215.0\n5000.0\n7.63220\n-25.46569\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n2\n41.4\n18.5\n202.0\n3875.0\n9.59462\n-25.42621\n0\n0\n1\n1\n0\n1\n0\n1\n\n\n3\n39.0\n18.7\n185.0\n3650.0\n9.22033\n-26.03442\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n4\n50.6\n19.4\n193.0\n3800.0\n9.28153\n-24.97134\n0\n1\n0\n1\n1\n0\n0\n1\n\n\n5\n33.1\n16.1\n178.0\n2900.0\n9.04218\n-26.15775\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n269\n41.1\n17.5\n190.0\n3900.0\n8.94365\n-26.06943\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n270\n45.4\n14.6\n211.0\n4800.0\n8.24515\n-25.46782\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n271\n36.2\n17.2\n187.0\n3150.0\n9.04296\n-26.19444\n0\n0\n1\n1\n1\n0\n1\n0\n\n\n272\n50.0\n15.9\n224.0\n5350.0\n8.20042\n-26.39677\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n273\n48.2\n14.3\n210.0\n4600.0\n7.68870\n-25.50811\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nExplore (Tables and Plots)\n\n# Show species by island \ntrain.groupby(['Species', 'Island'])['Region'].agg('count')\n\nSpecies                                    Island   \nAdelie Penguin (Pygoscelis adeliae)        Biscoe        35\n                                           Dream         41\n                                           Torgersen     42\nChinstrap penguin (Pygoscelis antarctica)  Dream         56\nGentoo penguin (Pygoscelis papua)          Biscoe       101\nName: Region, dtype: int64\n\n\nThis table shows the penguin species and the islands that they were found in. The results show that the Adelie penguins were found in all three islands while the Chinstrap penguins and the Gentoo penguins were found in Dream Island and Biscoe Island, respectively. This may be a good feature to train our model with because we can eliminate penguins depending on which island they were found in. For example, if a penguin were to be found in Torgersen Island, then there is a 100% chance it would be an Adelie penguin based on the table above. If a penguin were to be found in Biscoe Island, then it could either be an Adelie penguin or a Gentoo penguin but not a Chinstrap penguin.\n\n# Shown species by average culmen length, culmen depth, flipper length, and body mass\ntrain.groupby([\"Species\", \"Sex\"])[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\",\n                                  \"Body Mass (g)\"]].mean()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n37.100000\n17.645614\n187.719298\n3337.280702\n\n\nMALE\n40.458182\n19.116364\n192.690909\n4020.454545\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n46.424138\n17.641379\n191.551724\n3514.655172\n\n\nMALE\n51.185185\n19.303704\n199.666667\n3936.111111\n\n\nGentoo penguin (Pygoscelis papua)\n.\n44.500000\n15.700000\n217.000000\n4875.000000\n\n\nFEMALE\n45.600000\n14.242857\n212.928571\n4677.976190\n\n\nMALE\n49.592593\n15.687037\n221.462963\n5502.314815\n\n\n\n\n\n\n\nThis table organizes the average culmen length, culmen depth, flipper length, and body mass by penguin species. The results indicate that the penguin species with the highest average culmen length and depth by sex is the Chinstrap penguin. This can be a useful feature to train the model with because we can label the penguin with the greatest culmen length and depth as the Chinstrap penguin followed by the Gentoo penguin.\n\n# Create plot \nsns.set_theme()\n\nsns.relplot(data = train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n    title=\"Culmen Length vs. Body Mass\")\n\n#sns.relplot(data = train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n#    title=\"Culmen Depth vs. Body Mass\")\n\n#sns.relplot(data = train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n#    title=\"Flipper Length vs. Body Mass\")\n\n\n\n\nIn this experiment, I plotted body mass against culmen length for each penguin species. Based on the plot, we can see that Adelie penguins tend to have the smallest body mass as well as the smallest culmen length. Gentoo penguins generally have the same culmen length as Chinstrap penguins but Gentoo penguins are larger in terms of body mass.\n\n\nModel (Choosing Features)\nHere, I use the code provided in the blog post guide to help me select the most useful features. I am using logistic regression, decision tree classifier, and random forest classifier to see which features result in a 1.0 accuracy.\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nLR = LogisticRegression()\nDTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier()\n\n\nLogistic Regression\n\n# Logistic Regression\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    LR.fit(X_train[cols], y_train)\n    print(LR.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.96484375\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.92578125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n0.921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.80078125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.7890625\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.6640625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.95703125\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n0.91796875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.796875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.78515625\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.7421875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n0.859375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.8828125\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.85546875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.78515625\n\n\n\n\nDecision Tree Classifier\n\n# Decision Tree Classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    DTC.fit(X_train[cols], y_train)\n    print(DTC.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.9921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.9765625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.9921875\n\n\n\n\nRandom Forest Classifier\n\n# Random Forest Classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    RFC.fit(X_train[cols], y_train)\n    print(RFC.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.9921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.9765625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.9921875\n\n\nI will be using these features of the decision tree classifier: ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Body Mass (g)’ because they give us in 1.0 accuracy. Below is the proof that the score of the decision tree classifier using these features results in 1.0.\n\n# Accuracy using Decision Tree Classifier on features 'cols'\ncols = ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\nDTC.fit(X_train[cols], y_train)\nprint(\"Accuracy: \" + str(DTC.score(X_train[cols], y_train)))\n\nAccuracy: 1.0\n\n\n\n\n\nEvaluate and Testing\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\ncols = ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n\nDTC_score = DTC.score(X_test[cols], y_test)\nprint(\"Score on test data using Decision Tree Classifier: \" + str(DTC_score))\n\nScore on test data using Decision Tree Classifier: 0.8970588235294118\n\n\n\n# Helper function for plotting (from blog post guide)\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n# Plot Culmen Length vs. Body Mass\nDTC.fit(X_test[cols], y_test)\n\nplot_regions(DTC, X_test[cols], y_test)"
  },
  {
    "objectID": "posts/TimnitGebru/TG_final.html",
    "href": "posts/TimnitGebru/TG_final.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "About Dr. Timnit Gebru\nDr. Timnit Gebru is a computer scientist who specializes machine learning and artificial intelligence. She cofounded a research organization alongside Rediet Abebe known as Black in AI, which seeks to speak out about the lack of diversity in the field of artificial intelligence. Many of her works address racial discrimination and gender bias. Dr. Gebru coauthored a paper wher research showed that facial recognition was less accurate in identifying people of color and women. This would mean that there would be discrimination against these groups. This led to another research by other computer scientists and found that in facial recognition, black women were 35% less likely to be recognized than white men, further proving Dr. Gebru’s argument. Dr. Gebru is a recognized voice in artificial intelligence because she continues to uncover the ethical issues of the field even if it negatively impacts her. For example, she and five others did an research and put their findings on a paper called “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” during her time at Google. Despite the opposition from the higer-ups, Gebru insisted that the paper be published, which led to her departure from the company. Regardless of these things, she continues to move forward with the effort the address social injustices in the field of AI through research and starting organizations like Black in AI and the Distributed Artificial Intelligence Research Institute.\n\n\nTutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision\nIn Dr. Gebru’s talk, she speaks about how the field of computer vision is negatively affecting certain groups of people. She stated that computer vision is being utilized for things like the hiring process, surveillance, and policing. Although computer vision has its pros, there are also cons. For example, Dr. Gebru points out the use of face recognition in Maryland, where they used it to identify the Annapolis Capital Gazette shooter, and while this was a benefit, the police started to use it on photos to identify individuals at the Freddit Gray protests. In addition to surveillance, there are racial issues of computer vision. She noted that there were high disparity between black women and white men. Facial recognition was accurate when identifying white men but were much less accurate when identifying black women. Some companies believed that they did not have enough data to correctly identify certain groups of people such as people of the transgender community. As a result, they used videos of Transgender Youtubers without permission to enhance their data for facial/gender recognition.\nComputer vision algorithms can be useful, but it can cause a lot more harm than one might think.\n\nQuestion for Dr. Gebru\nWhat can companies do to prevent algorithmic biases in computer vision?\n\n\n\nSummary of Dr. Gebru’s Talk\nThe main focus on Dr. Gebru’s talk on Monday was about AGI (artificial general intelligence). In her talk, she briefly goes over the first wave of eugenics, movement that sought to improve the human population through sterilization, selective breeding, etc. and also the second wave of eugenics. Dr. Gebru argues that the current growth of AGI is correlated to the second wave of eugenics. She mentions that both the first and the second wave of eugenics (mostly the latter) are issues that are starting to surface as the field of AGI grows because these AGI models select the traits of humans that it deems helpful to the human population.\nDr. Gebru also explains that AGI is being used to create a utopian society. She describes that basic human needs will be met and the problems of the world such as poverty will be solved through the development in AGI. However, she argues that this utopian society that is promised through AGI will only benefit the rich. Another reason she links the development in AGI to second wave eugenics is due to its transhumanism ideology. Dr. Gebru notes that the field of AGI is being geared towards developing a society where humans and technology are integrated. Transhumanists believe that the integration of human and technology will improve society as a whole. Dr. Gebru raises concerns for this ideology and that only the privileged will be able to improve their living standards while others are left behind.\nI agree with the arguments of Dr. Gebru. Although I am fairly new to the ideas that Dr. Gebru has brought up in the talk, I believe I can agree with the fact that TESCREAL is ethically wrong even if they believe they are doing good for the future of the human population. AI may not seem like it is related to the second wave eugenics, but I can agree with Dr. Gebru with the fact that in the near future, their relation will become clearer.\n\n\nMy Thoughts\nAttending the talk by Dr. Gebru was a great experience and the topic was very interesting. Before attending the talk, I had never heard any of the TESCREAL terms, but after attending, it helped me aware of what is going on in the field of artificial intelligence and that it is being pushed in this direction. I became interested and researched on my own time about how the second wave of eugenics is related to AGI. Something that I was sort of aware was transhumanism; the integration of man and machine is something that I have always disagreed with and after the talk I became certain of my opinion."
  },
  {
    "objectID": "posts/Penguins/penguins1.html",
    "href": "posts/Penguins/penguins1.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer penguins data used in this blog post was collected by Dr. Kristen Gorman and the Palmer Station, which is a member of Long Term Ecological Research Network. The data contains physical measurements of three penguin species: Adelie, Chinstrap, and Gentoo.\n\nData Preparation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import matplotlib as plt\nfrom matplotlib.patches import Patch\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n27\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN46A1\nYes\n11/29/07\n44.5\n14.3\n216.0\n4100.0\nNaN\n7.96621\n-25.69327\nNaN\n\n\n1\nPAL0708\n22\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN41A2\nYes\n11/27/07\n45.1\n14.5\n215.0\n5000.0\nFEMALE\n7.63220\n-25.46569\nNaN\n\n\n2\nPAL0910\n124\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN67A2\nYes\n11/16/09\n41.4\n18.5\n202.0\n3875.0\nMALE\n9.59462\n-25.42621\nNaN\n\n\n3\nPAL0910\n146\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN82A2\nYes\n11/16/09\n39.0\n18.7\n185.0\n3650.0\nMALE\n9.22033\n-26.03442\nNaN\n\n\n4\nPAL0708\n24\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A2\nNo\n11/28/07\n50.6\n19.4\n193.0\n3800.0\nMALE\n9.28153\n-24.97134\nNaN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n1\n45.1\n14.5\n215.0\n5000.0\n7.63220\n-25.46569\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n2\n41.4\n18.5\n202.0\n3875.0\n9.59462\n-25.42621\n0\n0\n1\n1\n0\n1\n0\n1\n\n\n3\n39.0\n18.7\n185.0\n3650.0\n9.22033\n-26.03442\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n4\n50.6\n19.4\n193.0\n3800.0\n9.28153\n-24.97134\n0\n1\n0\n1\n1\n0\n0\n1\n\n\n5\n33.1\n16.1\n178.0\n2900.0\n9.04218\n-26.15775\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n269\n41.1\n17.5\n190.0\n3900.0\n8.94365\n-26.06943\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n270\n45.4\n14.6\n211.0\n4800.0\n8.24515\n-25.46782\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n271\n36.2\n17.2\n187.0\n3150.0\n9.04296\n-26.19444\n0\n0\n1\n1\n1\n0\n1\n0\n\n\n272\n50.0\n15.9\n224.0\n5350.0\n8.20042\n-26.39677\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n273\n48.2\n14.3\n210.0\n4600.0\n7.68870\n-25.50811\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nExplore (Tables and Plots)\n\n# Show species by island \ntrain.groupby(['Species', 'Island'])['Region'].agg('count')\n\nSpecies                                    Island   \nAdelie Penguin (Pygoscelis adeliae)        Biscoe        35\n                                           Dream         41\n                                           Torgersen     42\nChinstrap penguin (Pygoscelis antarctica)  Dream         56\nGentoo penguin (Pygoscelis papua)          Biscoe       101\nName: Region, dtype: int64\n\n\nThis table shows the penguin species and the islands that they were found in. The results show that the Adelie penguins were found in all three islands while the Chinstrap penguins and the Gentoo penguins were found in Dream Island and Biscoe Island, respectively. This may be a good feature to train our model with because we can eliminate penguins depending on which island they were found in. For example, if a penguin were to be found in Torgersen Island, then there is a 100% chance it would be an Adelie penguin based on the table above. If a penguin were to be found in Biscoe Island, then it could either be an Adelie penguin or a Gentoo penguin but not a Chinstrap penguin.\n\n# Shown species by average culmen length, culmen depth, flipper length, and body mass\ntrain.groupby([\"Species\", \"Sex\"])[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\",\n                                  \"Body Mass (g)\"]].mean()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n37.100000\n17.645614\n187.719298\n3337.280702\n\n\nMALE\n40.458182\n19.116364\n192.690909\n4020.454545\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n46.424138\n17.641379\n191.551724\n3514.655172\n\n\nMALE\n51.185185\n19.303704\n199.666667\n3936.111111\n\n\nGentoo penguin (Pygoscelis papua)\n.\n44.500000\n15.700000\n217.000000\n4875.000000\n\n\nFEMALE\n45.600000\n14.242857\n212.928571\n4677.976190\n\n\nMALE\n49.592593\n15.687037\n221.462963\n5502.314815\n\n\n\n\n\n\n\nThis table organizes the average culmen length, culmen depth, flipper length, and body mass by penguin species. The results indicate that the penguin species with the highest average culmen length and depth by sex is the Chinstrap penguin. This can be a useful feature to train the model with because we can label the penguin with the greatest culmen length and depth as the Chinstrap penguin followed by the Gentoo penguin.\n\n# Create plot \nsns.set_theme()\n\nsns.relplot(data = train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n    title=\"Culmen Length vs. Body Mass\")\n\n#sns.relplot(data = train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n#    title=\"Culmen Depth vs. Body Mass\")\n\n#sns.relplot(data = train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n#    title=\"Flipper Length vs. Body Mass\")\n\n\n\n\nIn this experiment, I plotted body mass against culmen length for each penguin species. Based on the plot, we can see that Adelie penguins tend to have the smallest body mass as well as the smallest culmen length. Gentoo penguins generally have the same culmen length as Chinstrap penguins but Gentoo penguins are larger in terms of body mass.\n\n\nModel (Choosing Features)\nHere, I use the code provided in the blog post guide to help me select the most useful features. I am using logistic regression, decision tree classifier, and random forest classifier to see which features result in a 1.0 accuracy.\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nLR = LogisticRegression()\nDTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier()\n\n\nLogistic Regression\n\n# Logistic Regression\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    LR.fit(X_train[cols], y_train)\n    print(LR.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.96484375\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.92578125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n0.921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.80078125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.7890625\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.6640625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.95703125\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n0.91796875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.796875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.78515625\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.7421875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n0.859375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.8828125\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.85546875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.78515625\n\n\n\n\nDecision Tree Classifier\n\n# Decision Tree Classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    DTC.fit(X_train[cols], y_train)\n    print(DTC.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.9921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.9765625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.9921875\n\n\n\n\nRandom Forest Classifier\n\n# Random Forest Classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    RFC.fit(X_train[cols], y_train)\n    print(RFC.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.9921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.96875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.9765625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.9921875\n\n\nI will be using these features of the decision tree classifier: ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Body Mass (g)’ because they give us in 1.0 accuracy. Below is the proof that the score of the decision tree classifier using these features results in 1.0.\n\n# Accuracy using Decision Tree Classifier on features 'cols'\ncols = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\nDTC.fit(X_train[cols], y_train)\nprint(\"Accuracy: \" + str(DTC.score(X_train[cols], y_train)))\n\nAccuracy: 1.0\n\n\n\n\n\nEvaluate and Testing\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\ncols = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n\nDTC_score = DTC.score(X_test[cols], y_test)\nprint(\"Score on test data using Decision Tree Classifier: \" + str(DTC_score))\n\nScore on test data using Decision Tree Classifier: 0.9558823529411765\n\n\n\n# Helper function for plotting (from blog post guide)\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 5))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"blue\", \"green\", \"red\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n    \n    # Titles for the plots\n    axarr[0].set_title('Biscoe Islands')\n    axarr[1].set_title('Dream Island')\n    axarr[2].set_title('Torgersen Island')\n      \n    plt.tight_layout()\n\n\n# Plot Culmen Length vs. Body Mass by different island\ncols = ['Culmen Length (mm)', 'Body Mass (g)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nDTC.fit(X_test[cols], y_test)\n\nplot_regions(DTC, X_test[cols], y_test)"
  },
  {
    "objectID": "posts/Project/model/test_xlnet.html",
    "href": "posts/Project/model/test_xlnet.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "!pip install -U transformers\n!pip install -U simpletransformers\n!pip install torch\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting transformers\n  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 58.3 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\nCollecting huggingface-hub&lt;1.0,&gt;=0.11.0 (from transformers)\n  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 27.2 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\nCollecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 (from transformers)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 103.5 MB/s eta 0:00:00\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.11.0-&gt;transformers) (2023.4.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.11.0-&gt;transformers) (4.5.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (1.26.15)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.12)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.4)\nInstalling collected packages: tokenizers, huggingface-hub, transformers\nSuccessfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting simpletransformers\n  Downloading simpletransformers-0.63.11-py3-none-any.whl (250 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 250.7/250.7 kB 6.6 MB/s eta 0:00:00\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.22.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.27.1)\nRequirement already satisfied: tqdm&gt;=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.65.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2022.10.31)\nRequirement already satisfied: transformers&gt;=4.6.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.28.1)\nCollecting datasets (from simpletransformers)\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 27.3 MB/s eta 0:00:00\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.10.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\nCollecting seqeval (from simpletransformers)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 5.9 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.12.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.3)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.13.3)\nCollecting wandb&gt;=0.10.32 (from simpletransformers)\n  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 68.1 MB/s eta 0:00:00\nCollecting streamlit (from simpletransformers)\n  Downloading streamlit-1.22.0-py2.py3-none-any.whl (8.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/8.9 MB 86.8 MB/s eta 0:00:00\nCollecting sentencepiece (from simpletransformers)\n  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 74.1 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers&gt;=4.6.0-&gt;simpletransformers) (3.12.0)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers&gt;=4.6.0-&gt;simpletransformers) (0.14.1)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers&gt;=4.6.0-&gt;simpletransformers) (23.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers&gt;=4.6.0-&gt;simpletransformers) (6.0)\nRequirement already satisfied: Click!=8.0.0,&gt;=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.10.32-&gt;simpletransformers) (8.1.3)\nCollecting GitPython!=3.1.29,&gt;=1.0.0 (from wandb&gt;=0.10.32-&gt;simpletransformers)\n  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 21.0 MB/s eta 0:00:00\nRequirement already satisfied: psutil&gt;=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.10.32-&gt;simpletransformers) (5.9.5)\nCollecting sentry-sdk&gt;=1.0.0 (from wandb&gt;=0.10.32-&gt;simpletransformers)\n  Downloading sentry_sdk-1.22.2-py2.py3-none-any.whl (203 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.3/203.3 kB 8.4 MB/s eta 0:00:00\nCollecting docker-pycreds&gt;=0.4.0 (from wandb&gt;=0.10.32-&gt;simpletransformers)\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\nCollecting pathtools (from wandb&gt;=0.10.32-&gt;simpletransformers)\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... done\nCollecting setproctitle (from wandb&gt;=0.10.32-&gt;simpletransformers)\n  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.10.32-&gt;simpletransformers) (67.7.2)\nRequirement already satisfied: appdirs&gt;=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.10.32-&gt;simpletransformers) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,&lt;5,&gt;=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb&gt;=0.10.32-&gt;simpletransformers) (3.20.3)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;simpletransformers) (1.26.15)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;simpletransformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;simpletransformers) (2.0.12)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;simpletransformers) (3.4)\nRequirement already satisfied: pyarrow&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;simpletransformers) (9.0.0)\nCollecting dill&lt;0.3.7,&gt;=0.3.0 (from datasets-&gt;simpletransformers)\n  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 13.2 MB/s eta 0:00:00\nCollecting xxhash (from datasets-&gt;simpletransformers)\n  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 25.8 MB/s eta 0:00:00\nCollecting multiprocess (from datasets-&gt;simpletransformers)\n  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 18.0 MB/s eta 0:00:00\nRequirement already satisfied: fsspec[http]&gt;=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;simpletransformers) (2023.4.0)\nCollecting aiohttp (from datasets-&gt;simpletransformers)\n  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 54.6 MB/s eta 0:00:00\nCollecting responses&lt;0.19 (from datasets-&gt;simpletransformers)\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;simpletransformers) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;simpletransformers) (2022.7.1)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;simpletransformers) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;simpletransformers) (3.1.0)\nRequirement already satisfied: altair&lt;5,&gt;=3.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (4.2.2)\nCollecting blinker&gt;=1.0.0 (from streamlit-&gt;simpletransformers)\n  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\nRequirement already satisfied: cachetools&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (5.3.0)\nCollecting importlib-metadata&gt;=1.4 (from streamlit-&gt;simpletransformers)\n  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (8.4.0)\nCollecting pympler&gt;=0.9 (from streamlit-&gt;simpletransformers)\n  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164.8/164.8 kB 22.4 MB/s eta 0:00:00\nRequirement already satisfied: rich&gt;=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (13.3.4)\nRequirement already satisfied: tenacity&lt;9,&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (8.2.2)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (0.10.2)\nRequirement already satisfied: typing-extensions&gt;=3.10.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (4.5.0)\nRequirement already satisfied: tzlocal&gt;=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (4.3)\nCollecting validators&gt;=0.2 (from streamlit-&gt;simpletransformers)\n  Downloading validators-0.20.0.tar.gz (30 kB)\n  Preparing metadata (setup.py) ... done\nCollecting pydeck&gt;=0.1.dev5 (from streamlit-&gt;simpletransformers)\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 36.0 MB/s eta 0:00:00\nRequirement already satisfied: tornado&gt;=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit-&gt;simpletransformers) (6.2)\nCollecting watchdog (from streamlit-&gt;simpletransformers)\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.1/82.1 kB 8.6 MB/s eta 0:00:00\nRequirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (1.4.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (1.54.0)\nRequirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib&lt;1.1,&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (1.0.0)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (3.4.3)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (0.7.0)\nRequirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (1.8.1)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (2.3.0)\nRequirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;simpletransformers) (0.40.0)\nRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair&lt;5,&gt;=3.2.0-&gt;streamlit-&gt;simpletransformers) (0.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair&lt;5,&gt;=3.2.0-&gt;streamlit-&gt;simpletransformers) (3.1.2)\nRequirement already satisfied: jsonschema&gt;=3.0 in /usr/local/lib/python3.10/dist-packages (from altair&lt;5,&gt;=3.2.0-&gt;streamlit-&gt;simpletransformers) (4.3.3)\nRequirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair&lt;5,&gt;=3.2.0-&gt;streamlit-&gt;simpletransformers) (0.12.0)\nRequirement already satisfied: six&gt;=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds&gt;=0.4.0-&gt;wandb&gt;=0.10.32-&gt;simpletransformers) (1.16.0)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;simpletransformers) (23.1.0)\nCollecting multidict&lt;7.0,&gt;=4.5 (from aiohttp-&gt;datasets-&gt;simpletransformers)\n  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 14.3 MB/s eta 0:00:00\nCollecting async-timeout&lt;5.0,&gt;=4.0.0a3 (from aiohttp-&gt;datasets-&gt;simpletransformers)\n  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting yarl&lt;2.0,&gt;=1.0 (from aiohttp-&gt;datasets-&gt;simpletransformers)\n  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 30.7 MB/s eta 0:00:00\nCollecting frozenlist&gt;=1.1.1 (from aiohttp-&gt;datasets-&gt;simpletransformers)\n  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 20.0 MB/s eta 0:00:00\nCollecting aiosignal&gt;=1.1.2 (from aiohttp-&gt;datasets-&gt;simpletransformers)\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting gitdb&lt;5,&gt;=4.0.1 (from GitPython!=3.1.29,&gt;=1.0.0-&gt;wandb&gt;=0.10.32-&gt;simpletransformers)\n  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.7 MB/s eta 0:00:00\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;simpletransformers) (0.3.0)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;simpletransformers) (4.9)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib&lt;1.1,&gt;=0.5-&gt;tensorboard-&gt;simpletransformers) (1.3.1)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata&gt;=1.4-&gt;streamlit-&gt;simpletransformers) (3.15.0)\nRequirement already satisfied: markdown-it-py&lt;3.0.0,&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich&gt;=10.11.0-&gt;streamlit-&gt;simpletransformers) (2.2.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich&gt;=10.11.0-&gt;streamlit-&gt;simpletransformers) (2.14.0)\nRequirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal&gt;=1.1-&gt;streamlit-&gt;simpletransformers) (0.1.0.post0)\nRequirement already satisfied: decorator&gt;=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators&gt;=0.2-&gt;streamlit-&gt;simpletransformers) (4.4.2)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard-&gt;simpletransformers) (2.1.2)\nCollecting smmap&lt;6,&gt;=3.0.1 (from gitdb&lt;5,&gt;=4.0.1-&gt;GitPython!=3.1.29,&gt;=1.0.0-&gt;wandb&gt;=0.10.32-&gt;simpletransformers)\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=3.0-&gt;altair&lt;5,&gt;=3.2.0-&gt;streamlit-&gt;simpletransformers) (0.19.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&lt;3.0.0,&gt;=2.2.0-&gt;rich&gt;=10.11.0-&gt;streamlit-&gt;simpletransformers) (0.1.2)\nRequirement already satisfied: pyasn1&lt;0.6.0,&gt;=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;simpletransformers) (0.5.0)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;1.1,&gt;=0.5-&gt;tensorboard-&gt;simpletransformers) (3.2.2)\nRequirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim-&gt;tzlocal&gt;=1.1-&gt;streamlit-&gt;simpletransformers) (2023.3)\nBuilding wheels for collected packages: seqeval, validators, pathtools\n  Building wheel for seqeval (setup.py) ... done\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=c4c8fdc78d36bddadeec7d59513e2c1ef34ee82e6f015c97c5a1aa689ed4039b\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n  Building wheel for validators (setup.py) ... done\n  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=b3623cdda028d6b3564fdde7e4810583052c30158eeff4d8103bdfbf9f411c2e\n  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n  Building wheel for pathtools (setup.py) ... done\n  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=63fcca267353c2b15df2601fc88f1c71387f9f38a8f37062afe163e0d2cef20d\n  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\nSuccessfully built seqeval validators pathtools\nInstalling collected packages: sentencepiece, pathtools, xxhash, watchdog, validators, smmap, setproctitle, sentry-sdk, pympler, multidict, importlib-metadata, frozenlist, docker-pycreds, dill, blinker, async-timeout, yarl, responses, pydeck, multiprocess, gitdb, aiosignal, seqeval, GitPython, aiohttp, wandb, streamlit, datasets, simpletransformers\nSuccessfully installed GitPython-3.1.31 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 blinker-1.6.2 datasets-2.12.0 dill-0.3.6 docker-pycreds-0.4.0 frozenlist-1.3.3 gitdb-4.0.10 importlib-metadata-6.6.0 multidict-6.0.4 multiprocess-0.70.14 pathtools-0.1.2 pydeck-0.8.1b0 pympler-1.0.1 responses-0.18.0 sentencepiece-0.1.99 sentry-sdk-1.22.2 seqeval-1.2.2 setproctitle-1.3.2 simpletransformers-0.63.11 smmap-5.0.0 streamlit-1.22.0 validators-0.20.0 wandb-0.15.2 watchdog-3.0.0 xxhash-3.2.0 yarl-1.9.2\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (3.25.2)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (16.0.3)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch) (2.1.2)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch) (1.3.0)\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv(\"/content/ReviewsWithVotes (2).csv\")\n\n# fix votes nan to 0\ndf[\"votes\"] = df[\"votes\"].fillna(0)\n\nhigh_quality_threshold = 0\n\ndf[\"imputed_quality\"] = (df[\"votes\"] &gt; high_quality_threshold)*1\n\n\n# convert boolean cols to int\ndf[\"again\"] = df[\"again\"].astype(int)\ndf[\"instructorEnjoyed\"] = df[\"instructorEnjoyed\"].astype(int)\ndf[\"instructorAgain\"] = df[\"instructorAgain\"].astype(int)\n\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nvotes\nreviewID\nreviewDate\ncourseID\ninstructorID\nsemester\ninMajorMinor\nwhyTake\nprimaryComponent\ntags\n...\nhours\nagain\ninstructorEffectiveness\ninstructorEnthusiasm\ninstructorAccommodationLevel\ninstructorEnjoyed\ninstructorAgain\nwasAuthorized\ncontent\nimputed_quality\n\n\n\n\n0\n3.0\n5cede120-8865-40eb-b20a-1815dd9eaa5a\n2022-12-28 02:16:11.988+00\nMATH0118\nC3177E57E2B4E380E08925E92F272599\nF22\nneither\nSpecific interest\n{Projects}\n{\"Chill and Relaxed\",\"Project Exams\"}\n...\n1\n1\n10\n10\n10\n1\n1\n0\nProf Malcolm-White is wonderful. Approachable ...\n1\n\n\n1\n3.0\n465f82cd-ccc0-4439-9fc7-2acbbc93bbd5\n2023-01-20 13:41:48.382+00\nECON0229\n7D4C6B2D9F24BDB5809D5947FC7338BC\nF22\nmajor\nElective for Major/Minor\n{Readings,Lectures,Papers}\n{\"Chill and Relaxed\",\"Constant Reading\",\"Easy ...\n...\n10\n1\n9\n10\n10\n1\n1\n1\nIn Econ Hist, Gregg is a phenomenal professor ...\n1\n\n\n2\n3.0\n7cd057fb-e17b-403a-8a32-91fa16d6abb7\n2022-12-18 21:32:13.937+00\nMATH0223\nD3CD48D2B8CAD51ACB0F48C000189315\nF21\nmajor\nRequired for Major/Minor\n{Lectures,Homework,Exams}\n{\"Lots of Homework\",\"Fair Grading\"}\n...\n10\n1\n8\n7\n8\n1\n1\n1\nProfessor Olinick does a good job of thoroughl...\n1\n\n\n3\n3.0\ne49d6b17-083c-4022-88c3-e76866b50313\n2022-11-14 08:22:50.083+00\nMATH0200\n775EF6B5D466E85B040334FBC42FF7AC\nF21\nminor\nRequired for Major/Minor\n{Exams}\n{Fast-Paced,\"Fair Grading\",\"Difficult Exams\"}\n...\n8\n1\n6\n9\n7\n1\n1\n0\nThe material is pretty hard. He gives you prob...\n1\n\n\n4\n3.0\ne14841bb-4e14-4b1a-8193-a3a7432ff6ba\n2022-11-08 15:09:31.648+00\nPHYS0155\nF16ADCB6066D31DAF05DD3D79133CD18\nF22\nneither\nSomeone recommended it\n{Exams,Homework}\n{\"Chill and Relaxed\",\"Lots of Homework\",\"Easy ...\n...\n3\n0\n8\n10\n8\n1\n0\n1\nProfessor Glikman is very enthusiastic about t...\n1\n\n\n\n\n\n5 rows × 23 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndef get_length(row):\n    return len(row[\"content\"].split(\" \"))\n\n\ndf[\"content_length\"] = df.apply(get_length, axis=1)\n\nprint(df[\"content_length\"].max())\nprint(df[\"content_length\"].mean())\nprint(df[\"content_length\"].median())\n\n335\n68.95376623376623\n58.0\n\n\n\nval_counts = df[\"imputed_quality\"].value_counts()\nprint(val_counts)\nprint(val_counts.max()/val_counts.sum())\n\n1    1036\n0     889\nName: imputed_quality, dtype: int64\n0.5381818181818182\n\n\n\n# splitting the data into train and test dataset\nX = df['content']\ny = df['imputed_quality']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\ntrain_df = pd.DataFrame(X_train)\ntrain_df['imputed_quality'] = y_train\n\ntest_df = pd.DataFrame(X_test)\ntest_df['imputed_quality'] = y_test\n\ntrain_df.shape, test_df.shape\n\n((1540, 2), (385, 2))\n\n\n\nprint(X_train)\n\n125     I really enjoyed this intro class and the prof...\n1616    Marion Wells is amazing and this course was on...\n971     I loved this class. The content was engaging, ...\n1695    For an intro-level class, he is a good profess...\n1796    Pete was in the process of restructuring his g...\n                              ...                        \n851     This was my favorite course this semester. Pro...\n701     Professor Greeley creates a fun and welcoming ...\n100     Professor Gauvin is great. She clearly explain...\n1106    The course was the best course I've taken in m...\n1782    Hector is great. He's passionate about the top...\nName: content, Length: 1540, dtype: object\n\n\n\nfrom sklearn.utils import resample\n#create two different dataframe of majority and minority class \ndf_majority = train_df[(train_df['imputed_quality']==0)] \ndf_minority = train_df[(train_df['imputed_quality']==1)] \n# upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,    # sample with replacement\n                                 n_samples= len(df_majority.index), # to match majority class\n                                 random_state=42)  # reproducible results\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_minority_upsampled, df_majority])\n\n\nfrom simpletransformers.classification import ClassificationModel, ClassificationArgs\nimport pandas as pd\nimport logging\nimport sklearn\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# They are lot of arguments to play with\nargs = {\n   'output_dir': 'outputs/',\n   'cache_dir': 'cache/',\n   'fp16': True,\n   'fp16_opt_level': 'O1',\n   'max_seq_length': 256,\n   'train_batch_size': 48,\n   'eval_batch_size': 1,\n   'gradient_accumulation_steps': 1,\n   'num_train_epochs': 8,\n   'weight_decay': 0.01,\n   'learning_rate': 1e-6,\n   'adam_epsilon': 1e-8,\n   'warmup_ratio': 0.06,\n   'warmup_steps': 0,\n   'max_grad_norm': 1.0,\n   'logging_steps': 100,\n   'evaluate_during_training': True,\n   'evaluate_during_training_steps': 2000,\n   'save_steps': 2000,\n   'eval_all_checkpoints': True,\n   'use_tensorboard': True,\n   'overwrite_output_dir': True,\n   'reprocess_input_data': False,\n   'wandb_project': 'test_xlnet',\n   \"save_model_every_epoch\" : False,\n   \"model_args.train_custom_parameters_only\" : True,\n   \"model_args.custom_parameter_groups\" : [\n      {\n          \"params\": [\"classifier.weight\"],\n      },\n      {\n          \"params\": [\"classifier.bias\"],\n      },\n    ]\n}\n\n# Create a ClassificationModel\nmodel = ClassificationModel('xlnet', 'xlnet-base-cased', args=args) # You can set class weights by using the optional weight argument\n\n\n\n\n\n\n\nSome weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\n\n\n\n\n\n# Train the model\nmodel.train_model(train_df, eval_df = test_df, acc=sklearn.metrics.accuracy_score)\n\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\nwandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\nwandb: You can find your API key in your browser here: https://wandb.ai/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n ··········\n\n\nTracking run with wandb version 0.15.2\n\n\nRun data is saved locally in /content/wandb/run-20230510_130138-peh97fx8\n\n\nSyncing run celestial-wave-79 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/aidmmcmillan-team/test_xlnet\n\n\n View run at https://wandb.ai/aidmmcmillan-team/test_xlnet/runs/peh97fx8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(264,\n defaultdict(list,\n             {'global_step': [33, 66, 99, 132, 165, 198, 231, 264],\n              'train_loss': [0.773681640625,\n               0.52569580078125,\n               0.70001220703125,\n               0.4930419921875,\n               0.62615966796875,\n               0.65771484375,\n               0.62799072265625,\n               0.71148681640625],\n              'mcc': [0.0521799346023433,\n               0.19236381649425985,\n               0.23784347221951263,\n               0.2975677160969849,\n               0.33386214363996164,\n               0.3543284521397487,\n               0.3757684393135313,\n               0.37560794443703627],\n              'tp': [112, 150, 133, 145, 150, 155, 155, 156],\n              'tn': [91, 82, 106, 106, 108, 107, 111, 110],\n              'fp': [87, 96, 72, 72, 70, 71, 67, 68],\n              'fn': [95, 57, 74, 62, 57, 52, 52, 51],\n              'auroc': [0.52042284101395,\n               0.6360527601367856,\n               0.6928567551430278,\n               0.7188161537208925,\n               0.7407181240840254,\n               0.750067849970146,\n               0.7550480377788633,\n               0.7563914671877545],\n              'auprc': [0.5573248108872921,\n               0.6688196404380393,\n               0.7416846375128712,\n               0.7549920563875077,\n               0.7834078178253042,\n               0.7890923369788849,\n               0.7935940755184612,\n               0.7942137035907963],\n              'acc': [0.5272727272727272,\n               0.6025974025974026,\n               0.6207792207792208,\n               0.6519480519480519,\n               0.6701298701298701,\n               0.6805194805194805,\n               0.6909090909090909,\n               0.6909090909090909],\n              'eval_loss': [0.7011788504464286,\n               0.6633592481737013,\n               0.6422490107548702,\n               0.6292188768262987,\n               0.6172087434050325,\n               0.6101299335430195,\n               0.6057046469155845,\n               0.6043755073051948]}))\n\n\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.accuracy_score)\n\n/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n  warnings.warn(\n\n\n\n\n\nFinishing last run (ID:peh97fx8) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nTraining loss\n█▁\n\n\nacc\n▁▄▅▆▇███\n\n\nauprc\n▁▄▆▇████\n\n\nauroc\n▁▄▆▇████\n\n\neval_loss\n█▅▄▃▂▁▁▁\n\n\nfn\n█▂▅▃▂▁▁▁\n\n\nfp\n▆█▂▂▂▂▁▁\n\n\nglobal_step\n▁▂▃▃▄▅▆▆▇█\n\n\nlr\n█▁\n\n\nmcc\n▁▄▅▆▇███\n\n\ntn\n▃▁▇▇▇▇██\n\n\ntp\n▁▇▄▆▇███\n\n\ntrain_loss\n█▂▆▁▄▅▄▆\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\nTraining loss\n0.64896\n\n\nacc\n0.69091\n\n\nauprc\n0.79421\n\n\nauroc\n0.75639\n\n\neval_loss\n0.60438\n\n\nfn\n51\n\n\nfp\n68\n\n\nglobal_step\n264\n\n\nlr\n0.0\n\n\nmcc\n0.37561\n\n\ntn\n110\n\n\ntp\n156\n\n\ntrain_loss\n0.71149\n\n\n\n\n\n\n\n View run celestial-wave-79 at: https://wandb.ai/aidmmcmillan-team/test_xlnet/runs/peh97fx8Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230510_130138-peh97fx8/logs\n\n\nSuccessfully finished last run (ID:peh97fx8). Initializing new run:\n\n\nTracking run with wandb version 0.15.2\n\n\nRun data is saved locally in /content/wandb/run-20230510_131135-on09nbeu\n\n\nSyncing run smart-snow-80 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/aidmmcmillan-team/test_xlnet\n\n\n View run at https://wandb.ai/aidmmcmillan-team/test_xlnet/runs/on09nbeu\n\n\n\nresult\n\n{'mcc': 0.37560794443703627,\n 'tp': 156,\n 'tn': 110,\n 'fp': 68,\n 'fn': 51,\n 'auroc': 0.7563914671877545,\n 'auprc': 0.7942137035907963,\n 'acc': 0.6909090909090909,\n 'eval_loss': 0.6043755073051948}\n\n\n\ntest_content = test_df['content'].values.tolist()\npredictions, raw_outputs = model.predict(test_content)\nprint(sklearn.metrics.classification_report(test_df['imputed_quality'], predictions))\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.68      0.62      0.65       178\n           1       0.70      0.75      0.72       207\n\n    accuracy                           0.69       385\n   macro avg       0.69      0.69      0.69       385\nweighted avg       0.69      0.69      0.69       385\n\n\n\n\nprint(sklearn.metrics.confusion_matrix(test_df['imputed_quality'], predictions))\n\n[[110  68]\n [ 51 156]]\n\n\n\nprint(test_df['content'])\n\n22      Prof Collaer is an amazing instructor. She is ...\n390     Ways of seeing is usually a small class. This ...\n172     Professor García is such a kind, thoughtful an...\n636     Eco Evo with dave Allen was a very easy course...\n1719    Very real perspective into Con Bio, although a...\n                              ...                        \n1495    Prof Rao is a great teacher. I enjoyed his sta...\n3       The material is pretty hard. He gives you prob...\n515     Gerke is very chill and knowledgeable. A lot o...\n88      Being almost entirely project based, 401 is qu...\n319     I found this class very interesting and prof J...\nName: content, Length: 385, dtype: object\n\n\n\ntest_content = [[\"DO NOT TAKE THIS COURSE IF YOU VALUE HAVING A LIFE. The assignments are basically impossible and not even original. Every assignment has a deadline of 1 week to write enormous amounts of code and tests for that code. It is terrible.\"],\n                [\"Prof Sanches is WONDERFUL. I cannot express that enough. She is so kind, smart, funny, passionate, and accommodating. The course material is really interesting and she is very knowledgeable. There are readings and reading responses for very class, but she is generally flexible with deadlines. I haven’t gone through a single class without laughing. She’s an incredible professor and I definitely recommend the course, although it is not necessarily light on assignments.\"],\n                [\"Ajay Verghese is a great professor - he makes things very clear and is very accommodating. The only reason I didn't love this class is because of the subject material. I took it to satisfy the poli sci methods requirement\"],\n                [\"This class is good. It is a lot of work but if you put in the effort you can get an A. I would reccomend\"],\n                [\"This has been one of the best classes I've taken at Middlebury. Honestly, I couldn't reccomend it more. Professor Chodrow was very understanding and I loved his 'ungraded' grading style. In this class you will learn all you need to know about machine learning, from linear regression all the way to deep learning. Chodrow is great at explaining dificult concepts. There are weekly blog posts that will take 8-10 hours a week so be prepared, but the amount you learn from them makes it worth it. This class is a must in the Comp Sci department.\"],\n                [\"This has been one of the best classes I've taken at Middlebury. Honestly, I couldn't reccomend it more. Professor Chodrow was very understanding and I loved his 'ungraded' grading style. In this class you will learn all you need to know about machine learning, from linear regression all the way to deep learning. Chodrow is great at explaining dificult concepts. There are weekly blog posts that will take 8-10 hours a week so be prepared, but the amount you learn from them makes it worth it. This class is a must in the Comp Sci department.\"]]\npredictions1, raw_outputs = model.predict(test_content)\nprint(predictions1)\n\n\n\n\n[1, 1, 1, 1, 1, 0]"
  },
  {
    "objectID": "posts/Project/model/lexicalrichness.html",
    "href": "posts/Project/model/lexicalrichness.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport numpy as np\n\n# Read in the CSV file\ndf = pd.read_csv('ReviewsWithVotes1.csv')\n\n\n# fix votes nan to 0\ndf[\"votes\"] = df[\"votes\"].fillna(0)\n# df[\"votes\"] = df[\"votes\"] - 1 # remove self votes\n\n# # normalize votes\n# df[\"votes\"] = df[\"votes\"] / df[\"votes\"].max()\n\n\n# high_quality_threshold = 0.45\n\n# df[\"imputed_quality\"] = (df[\"votes\"] &gt;= high_quality_threshold)*1\ndf[\"imputed_quality\"] = (df[\"votes\"] &gt; 1) * 1 # sum of annotators votes &gt; 1 ie (2 or 3)\n\n# TODO:  Setting threshold to 0 (1,2,3 annotators) greatly improves performance =&gt; we should annotate more!\n\n\n# convert boolean cols to int\ndf[\"again\"] = df[\"again\"].astype(int)\ndf[\"instructorEnjoyed\"] = df[\"instructorEnjoyed\"].astype(int)\ndf[\"instructorAgain\"] = df[\"instructorAgain\"].astype(int)\n\n\n\n# Define a function to calculate TTR\ndef calculate_ttr(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens_val = []\n    \n    for token in tokens:\n        if token not in stop_words:\n            tokens_val.append(token)\n    \n    # Calculate TTR\n    ttr = len(set(tokens_val)) / len(tokens_val)\n    return ttr\n\n\n# Define a function to calculate Herdan's C\ndef calculate_herdan(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens_val = []\n    \n    for token in tokens:\n        if token not in stop_words:\n            tokens_val.append(token)\n    \n    # Calculate Herdan's C\n    herdan = np.log(len(set(tokens_val))) / np.log(len(tokens_val))\n    return herdan\n\n\n# Define a function to calculate RTTR\ndef calculate_rttr(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens_val = []\n    \n    for token in tokens:\n        if token not in stop_words:\n            tokens_val.append(token)\n    \n    # Calculate RTTR\n    rttr = len(set(tokens_val)) / np.sqrt(len(tokens_val))\n    return rttr\n\n\n# Define a function to calculate CTTR\ndef calculate_cttr(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    \n    # Remove stop words\n    \n    stop_words = set(stopwords.words('english'))\n    tokens_val = []\n    \n    for token in tokens:\n        if token not in stop_words:\n            tokens_val.append(token)\n    \n    # Calculate CTTR\n    cttr = len(set(tokens_val)) / np.sqrt(2*len(tokens_val))\n    return cttr\n\n\n# Define a function to calculate Summer's\ndef calculate_summer(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens_val = []\n    \n    for token in tokens:\n        if token not in stop_words:\n            tokens_val.append(token)\n    \n    # Calculate Summer's method\n    summer = np.log(np.log(len(set(tokens_val)))) / np.log(np.log(len(tokens_val)))\n    return summer\n\n\n# Define a function to calculate Maas\ndef calculate_maas(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens_val = []\n    \n    for token in tokens:\n        if token not in stop_words:\n            tokens_val.append(token)\n    \n    # Calculate Maas method\n    maas = (np.log(len(tokens_val)) - np.log(len(set(tokens_val)))) / (np.log(len(tokens_val))**2)\n                                                                       \n    return maas\n\n\n# Define a function to calculate Mean Word Frequency\ndef calculate_mwf(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    \n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens_val = []\n    \n    for token in tokens:\n        if token not in stop_words:\n            tokens_val.append(token)\n    \n    # Calculate Dugast method\n    mwf = len(tokens_val) / len(set(tokens_val))\n    return mwf\n\n\n# Create a new column with TTR values\ndf['ttr'] = df['content'].apply(calculate_ttr)\n\n# TTR by imputed_quality\nplt.figure()\nplt.hist(df[df['imputed_quality'] == 1]['ttr'], bins=100, alpha=0.5, label='imputed_quality = 1')\nplt.hist(df[df['imputed_quality'] == 0]['ttr'], bins=100, alpha=0.5, label='imputed_quality = 0')\nplt.legend(loc='upper left')\n\nplt.xlabel('TTR')\nplt.show()\n\n\n\n\n\n# Create a new column with CTTR values\ndf['cttr'] = df['content'].apply(calculate_cttr)\n\n# CTTR by imputed_quality\nplt.figure()\nplt.hist(df[df['imputed_quality'] == 1]['cttr'], bins=100, alpha=0.5, label='imputed_quality = 1')\nplt.hist(df[df['imputed_quality'] == 0]['cttr'], bins=100, alpha=0.5, label='imputed_quality = 0')\nplt.legend(loc='upper right')\n\nplt.xlabel('CTTR')\nplt.show()\n\n\n\n\n\n# Create a new column with Herdans values\ndf['herdan'] = df['content'].apply(calculate_herdan)\n\n# Herdan's C by imputed_quality\nplt.figure()\nplt.hist(df[df['imputed_quality'] == 1]['herdan'], bins=100, alpha=0.5, label='imputed_quality = 1')\nplt.hist(df[df['imputed_quality'] == 0]['herdan'], bins=100, alpha=0.5, label='imputed_quality = 0')\nplt.legend(loc='upper left')\n\nplt.xlabel('Herdan\\'s C')\nplt.show()\n\n\n\n\n\n# Create a new column with RTTR values\ndf['rttr'] = df['content'].apply(calculate_rttr)\n\n# RTTR by imputed_quality\nplt.figure()\nplt.hist(df[df['imputed_quality'] == 1]['rttr'], bins=100, alpha=0.5, label='imputed_quality = 1')\nplt.hist(df[df['imputed_quality'] == 0]['rttr'], bins=100, alpha=0.5, label='imputed_quality = 0')\nplt.legend(loc='upper right')\n\nplt.xlabel('RTTR')\nplt.show()\n\n\n\n\n\n# Create a new column with Summer values\ndf['summer'] = df['content'].apply(calculate_summer)\n\n# Summer by imputed_quality\nplt.figure()\nplt.hist(df[df['imputed_quality'] == 1]['summer'], bins=100, alpha=0.5, label='imputed_quality = 1')\nplt.hist(df[df['imputed_quality'] == 0]['summer'], bins=100, alpha=0.5, label='imputed_quality = 0')\nplt.legend(loc='upper left')\n\nplt.xlabel('Summer')\nplt.show()\n\n\n\n\n\n# Create a new column with Maas values\ndf['maas'] = df['content'].apply(calculate_maas)\n\n# Maas by imputed_quality\nplt.figure()\nplt.hist(df[df['imputed_quality'] == 1]['maas'], bins=100, alpha=0.5, label='imputed_quality = 1')\nplt.hist(df[df['imputed_quality'] == 0]['maas'], bins=100, alpha=0.5, label='imputed_quality = 0')\nplt.legend(loc='upper right')\n\nplt.xlabel('Maas')\nplt.show()\n\n\n\n\n\n# Create a new column with mean word frequency values\ndf['mwf'] = df['content'].apply(calculate_mwf)\n\n# Mean Word Frequency by imputed_quality\nplt.figure()\nplt.hist(df[df['imputed_quality'] == 1]['mwf'], bins=100, alpha=0.5, label='imputed_quality = 1')\nplt.hist(df[df['imputed_quality'] == 0]['mwf'], bins=100, alpha=0.5, label='imputed_quality = 0')\nplt.legend(loc='upper right')\n\nplt.xlabel('Mean Word Frequency')\nplt.show()"
  },
  {
    "objectID": "posts/Project/model/test.html",
    "href": "posts/Project/model/test.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"data/ReviewsWithVotes.csv\")\n\n# fix votes nan to 0\ndf[\"votes\"] = df[\"votes\"].fillna(0)\n# df[\"votes\"] = df[\"votes\"] - 1 # remove self votes\n\n# # normalize votes\n# df[\"votes\"] = df[\"votes\"] / df[\"votes\"].max()\n\n\n# high_quality_threshold = 0.45\n\n# df[\"imputed_quality\"] = (df[\"votes\"] &gt;= high_quality_threshold)*1\ndf[\"imputed_quality\"] = (df[\"votes\"] &gt; 0) * 1 # sum of annotators votes &gt; 1 ie (2 or 3)\n# TODO: decide if this thereshold should be 0 or 1\n# TODO:  Setting threshold to 0 (1,2,3 annotators) greatly improves performance =&gt; we should annotate more!\n\n\n# convert boolean cols to int\ndf[\"again\"] = df[\"again\"].astype(int)\ndf[\"instructorEnjoyed\"] = df[\"instructorEnjoyed\"].astype(int)\ndf[\"instructorAgain\"] = df[\"instructorAgain\"].astype(int)\n\n\n\n\ndef get_vectorizer(df, text_column):\n    \"\"\"\n    Vectorize the text column\n    \"\"\"\n    vect = TfidfVectorizer(\n        max_features=1000,\n        max_df=0.8,\n        min_df=0.1,\n        stop_words=\"english\",\n    )\n    vect.fit_transform(df[text_column])\n    return vect\n\n\n\n\n\n\n# Q: whats the latest date in the dataset?\n\nprint(df[\"reviewDate\"].max())\n\n# Q: how many reviews are there in the dataset?\n\nprint(len(df))\n\n2023-04-21 18:09:01.856+00\n1925\n\n\n\ndf[\"imputed_quality\"].value_counts()\n\n1    1036\n0     889\nName: imputed_quality, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\n\n# plt.hist(df[\"votes\"], bins=6)\n\n\nplt.scatter(df[\"instructorEnthusiasm\"], df[\"votes\"]) # these won't work\n\n\n&lt;matplotlib.collections.PathCollection at 0x1f688dc16f0&gt;\n\n\n\n\n\n\nhigh_quality_df = df[df[\"imputed_quality\"] == 1]\n\n\nv = get_vectorizer(high_quality_df, \"content\")\n\nX = v.transform(high_quality_df[\"content\"])\n\nprint(v.get_feature_names_out())\n\n\nbad_quality_df = df[df[\"votes\"] &lt; 0]\n\nv_bad = get_vectorizer(bad_quality_df, \"content\")\nX_bad = v_bad.transform(bad_quality_df[\"content\"])\n\nprint(v_bad.get_feature_names_out())\n\n\n\n['assignments' 'class' 'classes' 'content' 'course' 'definitely'\n 'difficult' 'discussion' 'easy' 'engaging' 'enjoyed' 'exams' 'extremely'\n 'final' 'fun' 'good' 'great' 'hard' 'homework' 'hours' 'interesting'\n 'just' 'learn' 'lectures' 'like' 'lot' 'material' 'overall' 'papers'\n 'pretty' 'prof' 'professor' 'reading' 'readings' 'really' 'recommend'\n 'semester' 'students' 'super' 'taking' 'teaching' 'think' 'time'\n 'understanding' 'week' 'work']\n['boring' 'class' 'course' 'definitely' 'difficult' 'does' 'easy' 'exams'\n 'good' 'great' 'hard' 'homework' 'hours' 'interesting' 'just' 'lectures'\n 'like' 'lot' 'material' 'nice' 'pretty' 'professor' 'really' 'recommend'\n 'students' 'super' 'took' 'work']\n\n\n\n# y = df[\"imputed_quality\"]\n\n# x_df = pd.DataFrame(X.toarray(), columns=v.get_feature_names_out())\n# # x_df[\"imputed_quality\"] = labels\n\n# from sklearn.model_selection import train_test_split\n\n\n# X_train, X_test, y_train, y_test = train_test_split(x_df, y, test_size=0.15)\n\n# from sklearn.linear_model import LogisticRegression\n\n# lr = LogisticRegression(penalty=\"l2\")\n# lr.fit(X_train, y_train)\n\n# y_pred = lr.predict(X_test)\n\n# print(classification_report(y_test, y_pred))\n\n\n\n\n\n\n\nsliders = [\n    \"rating\",\n    \"difficulty\",\n    \"value\",\n    \"instructorEffectiveness\",\n    \"instructorAccommodationLevel\",\n    \"instructorEnthusiasm\",\n]\n\ndef get_default_slider_mse(row):\n    return np.mean((row[sliders] - 5)**2)\n\ndf[\"default_mse\"] = df.apply(get_default_slider_mse, axis=1)\n\n\ndef get_review_content_length(row):\n    return (len(row[\"content\"]) - 200)/200\n\ndf[\"content_length\"] = df.apply(get_review_content_length, axis=1)\n\n\ndef review_content_larger_than_250(row):\n    return (len(row[\"content\"]) &gt; 250) * 1\n\ndf[\"content_larger_than_250\"] = df.apply(review_content_larger_than_250, axis=1)\n\n\n# quality vs content_larger_than_250\n\n# df[\"content_larger_than_250\"].value_counts()\n\n# df[\"imputed_quality\"].value_counts()\n\ndf[df[\"content_larger_than_250\"] == False][\"imputed_quality\"].value_counts()\n\n0    373\n1    108\nName: imputed_quality, dtype: int64\n\n\n\nplt.figure()\n\n\nplt.scatter(df[\"content_length\"], df[\"votes\"])\n\nplt.show()\n\n\n\n\n\ndef get_average_sentence_length(row):\n    return len(row[\"content\"].split(\" \")) / len(row[\"content\"].split(\".\"))\n\n\ndf[\"average_sentence_length\"] = df.apply(get_average_sentence_length, axis=1)\n\n\nplt.figure()\n\nplt.scatter(df[\"average_sentence_length\"], df[\"votes\"])\n\nplt.show()\n\n\n\n\n\nslider_cols = [\"value\", \"difficulty\", \"rating\", \"instructorEffectiveness\", \"instructorEnthusiasm\", \"instructorAccommodationLevel\"]\n\ndef get_variance(vals):\n    return np.var(vals)\n\ncolumns = [\"value\", \"difficulty\", \"rating\", \"again\", \"instructorEffectiveness\", \"instructorEnthusiasm\", \"instructorAccommodationLevel\", \"instructorEnjoyed\", \"instructorAgain\"]\n\n\ndefault_slider_vals = {\n    \"value\": 5,\n    \"difficulty\": 5,\n    \"rating\": 5,\n    \"again\": 0,\n    \"instructorEffectiveness\": 5,\n    \"instructorEnthusiasm\": 5,\n    \"instructorAccommodationLevel\": 5,\n    \"instructorEnjoyed\": 0,\n    \"instructorAgain\": 0\n}\n\noverly_negative_vals = {\n    \"value\": 1,\n    \"difficulty\": 10,\n    \"rating\": 1,\n    \"again\": 0,\n    \"instructorEffectiveness\": 1,\n    \"instructorEnthusiasm\": 1,\n    \"instructorAccommodationLevel\": 1,\n    \"instructorEnjoyed\": 0,\n    \"instructorAgain\": 0\n}\n\noverly_positive_vals = {\n    \"value\": 10,\n    \"difficulty\": 1,\n    \"rating\": 10,\n    \"again\": 1,\n    \"instructorEffectiveness\": 10,\n    \"instructorEnthusiasm\": 10,\n    \"instructorAccommodationLevel\": 10,\n    \"instructorEnjoyed\": 1,\n    \"instructorAgain\": 1\n}\n\ndefault_slider_vals_list = [default_slider_vals[col] for col in columns]\noverly_negative_vals_list = [overly_negative_vals[col] for col in columns]\noverly_positive_vals_list = [overly_positive_vals[col] for col in columns]\n\ndef get_mse(vals, expected_vals):\n    return np.mean(np.square(vals-expected_vals))\n\n\ndf[\"default_mse\"] = df[columns].apply(get_mse, axis=1, args=(default_slider_vals_list,))\ndf[\"overly_negative_mse\"] = df[columns].apply(get_mse, axis=1, args=(overly_negative_vals_list,))\ndf[\"overly_positive_mse\"] = df[columns].apply(get_mse, axis=1, args=(overly_positive_vals_list,))\n\ndf[\"slider_variance\"] = df[slider_cols].apply(get_variance, axis=1)\n\n\n# readability\n# from readability import flesch_reading_ease, smog_index, dale_chall_readability_score\n\nfrom textstat.textstat import textstatistics\nimport math\n\n\nflesch_reading_ease = lambda x: textstatistics().flesch_reading_ease(x)\nsmog_index = lambda x: textstatistics().smog_index(x)\ndale_chall_readability_score = lambda x: textstatistics().dale_chall_readability_score(x)\n\ndef get_fre_readability_score(row):\n    return flesch_reading_ease(row[\"content\"])\n\ndef get_smog_readability_score(row):\n    return smog_index(row[\"content\"])\n\ndef get_dale_chall_readability_score(row):\n    return dale_chall_readability_score(row[\"content\"])\n\ndf[\"fre_readability\"] = df.apply(get_fre_readability_score, axis=1)\ndf[\"smog_readability\"] = df.apply(get_smog_readability_score, axis=1)\ndf[\"dale_chall_readability\"] = df.apply(get_dale_chall_readability_score, axis=1)\n\n\nimport math\n\ndef char_freq(text: str):\n    \"\"\"Calculates the frequency of each character in the text\"\"\"\n    freq = {}\n    for char in text:\n        freq[char] = freq.get(char, 0) + 1\n    return freq\n\ndef shannon_entropy(text: str):\n    \"\"\"Calculates the Shannon entropy of a text string\"\"\"\n    length = len(text)\n    freq = char_freq(text)\n\n    entropy = 0\n    for char in freq:\n        prob = freq[char] / length\n        entropy -= prob * math.log(prob, 2)\n\n    return entropy\n\n\ndef get_review_entropy(row):\n    return shannon_entropy(row[\"content\"])\n\ndf[\"entropy\"] = df.apply(get_review_entropy, axis=1)\n\n\ndef get_space_length_ratio(row):\n    return len(row[\"content\"].split(\" \")) / len(row[\"content\"])\n\ndef get_percent_difficult_words(row):\n    # percent longer than 9 characters (2x average)\n    return len([word for word in row[\"content\"].split(\" \") if len(word) &gt; 9]) / len(row[\"content\"].split(\" \"))\n\n\ndef get_count_period(row):\n    return len([char for char in row[\"content\"] if char == \".\"])\n\ndef get_count_exclamation(row):\n    return len([char for char in row[\"content\"] if char == \"!\"])\n\ndef get_count_question(row):\n    return len([char for char in row[\"content\"] if char == \"?\"])\n\ndef get_ratio_question_marks(row):\n    return get_count_question(row) / len(row[\"content\"])\n\ndef get_ratio_exclamation_marks(row):\n    return get_count_exclamation(row) / len(row[\"content\"])\n\n\ndef get_all_cap_words(row):\n    return len([word for word in row[\"content\"].split(\" \") if word.isupper() and len(word) &gt; 1])\n\ndef get_all_cap_words_ratio(row):\n    return get_all_cap_words(row) / len(row[\"content\"].split(\" \"))\n\n# Features:\n# Space to length ratio\n# Percent difficult words\n# Ratio of question marks\n# Ratio of exclamation marks\n\ndf[\"space_length_ratio\"] = df.apply(get_space_length_ratio, axis=1)\ndf[\"percent_difficult_words\"] = df.apply(get_percent_difficult_words, axis=1)\ndf[\"ratio_question_marks\"] = df.apply(get_ratio_question_marks, axis=1)\ndf[\"ratio_exclamation_marks\"] = df.apply(get_ratio_exclamation_marks, axis=1)\ndf[\"ratio_all_cap_words\"] = df.apply(get_all_cap_words_ratio, axis=1)\n\n\n\ndef word_freq(text: str):\n    \"\"\"Calculates the frequency of each word in the text\"\"\"\n    freq = {}\n    for word in text.split(\" \"):\n        freq[word] = freq.get(word, 0) + 1\n    return freq\n\ndef word_shannon_entropy(text: str):\n    \"\"\"Calculates the Shannon entropy of a text string\"\"\"\n    length = len(text.split(\" \"))\n    freq = word_freq(text)\n\n    entropy = 0\n    for word in freq:\n        prob = freq[word] / length\n        entropy -= prob * math.log(prob, 2)\n\n    return entropy\n\ndef get_review_word_entropy(row):\n    return word_shannon_entropy(row[\"content\"].lower())\n\ndf[\"word_entropy\"] = df.apply(get_review_word_entropy, axis=1)\ndf[\"word_diversity\"] = df[\"word_entropy\"] / df[\"content_length\"] #this can create inf\ndf[\"word_diversity\"] = df[\"word_diversity\"].replace([np.inf, -np.inf], np.nan)\ndf[\"word_diversity\"] = df[\"word_diversity\"].fillna(df[\"word_diversity\"].max())\n\n\nfrom datetime import datetime, date, timedelta\n\ndef convert_term_string_to_date(term):\n    # F22 -&gt; 2022-09-01\n    # S21 -&gt; 2021-02-01\n    # W21 -&gt; 2021-01-01\n\n    year = int(term[1:3])\n    year = 2000 + year #Y2K baby!\n    \n    t_obj = {\n        \"F\": date(year, 9, 1),\n        \"S\": date(year, 2, 1),\n        \"W\": date(year, 1, 1)\n    }\n\n    return t_obj[term[0]]\n\n\n\ndef time_between_course_and_review(row):\n    # example reviewDate: 2022-10-24 03:16:49.083+00\n    # only care aoout the date, not the time\n    format = \"%Y-%m-%d\"\n    time_string = row[\"reviewDate\"]\n    course_date = convert_term_string_to_date(row[\"semester\"])\n\n    date_string = time_string.split(\" \")[0]\n\n\n    review_date = datetime.strptime(date_string, format).date()\n\n    return ((review_date - course_date).days) / 365\n\n\n\ndf[\"time_between_course_and_review\"] = df.apply(time_between_course_and_review, axis=1)\n\n\n# Sentiment Analysis\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nanalyser = SentimentIntensityAnalyzer()\n\ndef get_sentiment_scores(row):\n    return analyser.polarity_scores(row[\"content\"])[\"compound\"]\n\ndef get_neutrality_scores(row):\n    return analyser.polarity_scores(row[\"content\"])[\"neu\"]\n\ndf[\"sentiment_scores\"] = df.apply(get_sentiment_scores, axis=1)\ndf[\"neutrality_scores\"] = df.apply(get_neutrality_scores, axis=1)\n\n\n# Polarity\n# a measure of if sentiment matches the ratings\n\ndef get_consistent_polarity(row):\n    same_sign = 1 if row[\"sentiment_scores\"] * (row[\"rating\"]-5) &gt; 0 else 0\n    magnitude_threshold = 0.2\n    same_magnitude = 1 if (abs(row[\"sentiment_scores\"]) - abs(row[\"rating\"]/10)) &lt; magnitude_threshold else 0\n    return same_sign * same_magnitude\n\ndef get_polarity(row):\n    return row[\"sentiment_scores\"] - ((row[\"rating\"]-5)/10)\n\ndf[\"consistent_polarity\"] = df.apply(get_consistent_polarity, axis=1)\ndf[\"polarity\"] = df.apply(get_polarity, axis=1)\n\n\n# KNN avg distance to nearest neighbor\n\nfrom sklearn.neighbors import NearestNeighbors\nimport math\n\nk = 5\n\ndef get_knn_avg_distance(df, row):\n\n    X = df[[\"sentiment_scores\", \"neutrality_scores\", \"consistent_polarity\", \"polarity\", \"fre_readability\", \"smog_readability\", \"dale_chall_readability\", \"entropy\", \"word_entropy\", \"word_diversity\", \"time_between_course_and_review\"]].to_numpy()\n\n    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X)\n\n    distances, indices = nbrs.kneighbors([row[[\"sentiment_scores\", \"neutrality_scores\", \"consistent_polarity\", \"polarity\", \"fre_readability\", \"smog_readability\", \"dale_chall_readability\", \"entropy\", \"word_entropy\", \"word_diversity\", \"time_between_course_and_review\"]].to_numpy()])\n    return distances[0].mean()\n\n\n\ndef get_knn_same_course_avg_distance(df, row):\n\n    columns = [\n        \"rating\",\n        \"difficulty\",\n        \"value\",\n        \"hours\",\n        \"again\",\n        \n        \"default_mse\",\n        \"content_length\",\n        \"sentiment_scores\",\n    ]\n\n    courseID = row[\"courseID\"]\n    X = df[df[\"courseID\"] == courseID][columns].to_numpy()\n\n    n_neighbors = min(k, len(X))\n\n    if n_neighbors == 0:\n        return math.nan\n\n    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(X)\n\n    distances, indices = nbrs.kneighbors([row[columns].to_numpy()])\n\n    return distances[0].mean()\n\n\n\n\ndf[\"knn_avg_distance\"] = df.apply(lambda row: get_knn_avg_distance(df, row), axis=1)\ndf[\"knn_same_course_avg_distance\"] = df.apply(lambda row: get_knn_same_course_avg_distance(df, row), axis=1)\n\n# replace knn_same_course_avg_distance with avg if nan or inf\ndf[\"knn_same_course_avg_distance\"] = df[\"knn_same_course_avg_distance\"].replace([np.inf, -np.inf], np.nan)\ndf[\"knn_same_course_avg_distance\"] = df[\"knn_same_course_avg_distance\"].fillna(df[\"knn_same_course_avg_distance\"].mean())\n\n\n\n\n# number of numbers\nimport re\n\n\ndef get_number_count(row):\n    return len(re.findall(r'\\d+', row[\"content\"]))\n\ndf[\"number_count\"] = df.apply(get_number_count, axis=1)\n\n\ngrade_re = re.compile(r'([A-F][+-]?)')\n\ndef mentions_grades(row):\n    grade_like_words = [\"grade\", \"curves\", ]\n    if any(word in row[\"content\"] for word in grade_like_words):\n        return 1\n    \n    if grade_re.search(row[\"content\"]):\n        return 1\n    \n    return 0\n\n\ndf[\"mentions_grades\"] = df.apply(mentions_grades, axis=1)\n\n\ndef lowercase(string):\n    return string.lower()\n\ndef mentions_prof_word(row):\n    prof_like_words = [\"prof\", \n                       \"professor\", \n                       \"teacher\", \n                       \"instructor\", \n                       \"lecturer\", \n                       \"tutor\", \n                       \"advisor\", \n                       \"dr\",\n                       \"laoshi\"\n                       ]\n    if any(word in lowercase(row[\"content\"]) for word in prof_like_words):\n        return 1\n    \n    return 0\n\n\ndf[\"mentions_prof_word\"] = df.apply(mentions_prof_word, axis=1)\n\n\n\ndef contains_pronouns(row):\n    pronoun_list = [\"she\", \"he\", \"they\", \"them\", \"her\", \"him\", \"his\", \"hers\", \"their\", \"theirs\"]\n    re_word_boundary = r'\\b%s\\b'\n    if any(re.search(re_word_boundary % word, lowercase(row[\"content\"])) for word in pronoun_list):\n        return 1\n\n    return 0\n\n\ndef contains_personal_pronouns(row):\n    pronoun_list = [\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"]\n    re_word_boundary = r'\\b%s\\b'\n    if any(re.search(re_word_boundary % word, lowercase(row[\"content\"])) for word in pronoun_list):\n        return 1\n\n    return 0\n\n\ndf[\"contains_pronouns\"] = df.apply(contains_pronouns, axis=1)\ndf[\"contains_personal_pronouns\"] = df.apply(contains_personal_pronouns, axis=1)\n\n\ndef get_lexical_density_estimate(row):\n    content_word_count = 0\n    words = row[\"content\"].split()\n    for word in words:\n        if len(word) &gt; 3:\n            content_word_count += 1\n\n    return content_word_count / len(words)\n\ndf[\"lexical_density_estimate\"] = df.apply(get_lexical_density_estimate, axis=1)\n\n\nfrom lexicalrichness import LexicalRichness\n\ndef get_ttr(row):\n    return LexicalRichness(row[\"content\"]).ttr\n\ndef get_mattr(row):\n    return LexicalRichness(row[\"content\"]).mattr(window_size=20)\n\ndef get_cttr(row):\n    return LexicalRichness(row[\"content\"]).cttr\n\ndef get_simpsond(row):\n    return LexicalRichness(row[\"content\"]).simpsond\n\ndef get_hdd(row):\n    return LexicalRichness(row[\"content\"]).hdd(draws=20)\n\ndef get_summer(row):\n    return LexicalRichness(row[\"content\"]).Summer\n\ndef get_yulek(row):\n    return LexicalRichness(row[\"content\"]).yulek\n\ndef get_yulei(row):\n    return LexicalRichness(row[\"content\"]).yulei\n\ndef get_maas(row):\n    return LexicalRichness(row[\"content\"]).Maas\n\n\n\ndf[\"ttr\"] = df.apply(get_ttr, axis=1) #USEFULL!\ndf[\"cttr\"] = df.apply(get_cttr, axis=1) #USEFULL!\ndf[\"maas\"] = df.apply(get_maas, axis=1)\n\n\n\n\n# df[\"summer\"] = df.apply(get_summer, axis=1)s\n# df[\"yulek\"] = df.apply(get_yulek, axis=1)\n# df[\"yulei\"] = df.apply(get_yulei, axis=1)\n# df[\"simpsond\"] = df.apply(get_simpsond, axis=1)\n# df[\"hdd\"] = df.apply(get_hdd, axis=1)\n# df[\"mattr\"] = df.apply(get_mattr, axis=1)\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"ttr\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"ttr\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot: xlabel='ttr', ylabel='Density'&gt;\n\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"cttr\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"cttr\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot: xlabel='cttr', ylabel='Density'&gt;\n\n\n\n\n\n\n# sns.kdeplot(df[df[\"imputed_quality\"] == 1][\"simpsond\"], fill=True, label=\"imputed_quality = 1\")\n# sns.kdeplot(df[df[\"imputed_quality\"] == 0][\"simpsond\"], fill=True, label=\"imputed_quality = 0\")\n\n\n# sns.kdeplot(df[df[\"imputed_quality\"] == 1][\"hdd\"], fill=True, label=\"imputed_quality = 1\")\n# sns.kdeplot(df[df[\"imputed_quality\"] == 0][\"hdd\"], fill=True, label=\"imputed_quality = 0\")\n\n\n# sns.kdeplot(df[df[\"imputed_quality\"] == 1][\"summer\"], fill=True, label=\"imputed_quality = 1\")\n# sns.kdeplot(df[df[\"imputed_quality\"] == 0][\"summer\"], fill=True, label=\"imputed_quality = 0\")\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"maas\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"maas\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot: xlabel='maas', ylabel='Density'&gt;\n\n\n\n\n\n\nprof_id_to_name_map = {}\n\ninstructor_df = pd.read_csv(\"../data/Instructor.csv\")\n\nfor index, row in instructor_df.iterrows():\n    prof_id_to_name_map[row[\"instructorID\"]] = row[\"name\"]\n    \n\ndef get_prof_name(row):\n    prof_id = row[\"instructorID\"]\n    if prof_id in prof_id_to_name_map:\n        return prof_id_to_name_map[prof_id]\n    return None\n\ndef mentions_prof_name(row):\n    prof_name = get_prof_name(row)\n    if prof_name is None:\n        return 0\n    alternate_names = [*prof_name.split(\" \")]\n    if any(word in lowercase(row[\"content\"]) for word in alternate_names):\n        return 1\n    # check for first name substring longer than 3 (nickname-like)\n    for window in range(3, len(prof_name.split(\" \")[0])):\n        if prof_name.split(\" \")[0][:window] in lowercase(row[\"content\"]):\n            return 1\n    \n    return 0\n\n\ndf[\"mentions_prof_name\"] = df.apply(mentions_prof_name, axis=1)\n    \n\n\n\ndef slider_diff_from_base(row, slider):\n    return abs(row[slider] - 5)\n\n\ndiff_columns = [\n    \"rating\",\n    \"difficulty\",\n    \"value\",\n    \"instructorEnthusiasm\",\n    \"instructorEffectiveness\"\n]\n\n# rating diff\n#df[\"rating_diff\"] = df.apply(lambda row: slider_diff_from_base(row, \"rating\"), axis=1)\n\nfor column in diff_columns:\n    df[column+\"_diff\"] = df.apply(lambda row: slider_diff_from_base(row, column), axis=1)\n\n\n# kde plot of each diff column\n\nfor column in diff_columns:\n    diff_col = column+\"_diff\"\n    sns.kdeplot(df[df[\"imputed_quality\"] == 1][diff_col], fill=True, label=\"imputed_quality = 1\")\n    sns.kdeplot(df[df[\"imputed_quality\"] == 0][diff_col], fill=True, label=\"imputed_quality = 0\")\n    corr_with_imputed_quality = df[[\"imputed_quality\", diff_col]].corr().iloc[0,1]\n    print(f\"{diff_col} corr with imputed_quality: {corr_with_imputed_quality}\")\n    plt.title(column)\n    plt.show()\n\nrating_diff corr with imputed_quality: 0.1384959413532283\ndifficulty_diff corr with imputed_quality: -0.03911119613324394\nvalue_diff corr with imputed_quality: 0.09872790544291811\ninstructorEnthusiasm_diff corr with imputed_quality: 0.12521167571412636\ninstructorEffectiveness_diff corr with imputed_quality: 0.1148492823053899\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# sentiment_scores\n\nplt.figure()\n\nplt.scatter(df[\"sentiment_scores\"], df[\"votes\"])\n\nplt.show()\n\n\n\n\n\n# entropy\n\nplt.figure()\n\nplt.scatter(df[\"knn_avg_distance\"], df[\"votes\"])\n\nplt.show()\n\n\n\n\n\n# entropy\n\nplt.figure()\n\nplt.scatter(df[\"entropy\"], df[\"votes\"])\n\nplt.show()\n\n\n\n\n\n# word entropy\n\nplt.figure()\n\nplt.scatter(df[\"word_entropy\"], df[\"votes\"])\n\nplt.show()\n\n\n\n\n\n# hist of content length\n\nplt.figure()\n\nplt.hist(df[\"content_length\"], bins=100)\n\nplt.show()\n\n\n\n\n\n# hist of content length x imputed_quality\n\nplt.figure()\n\nplt.hist(df[df[\"imputed_quality\"] == 1][\"content_length\"], bins=100, alpha=0.5, label=\"imputed_quality = 1\")\nplt.hist(df[df[\"imputed_quality\"] == 0][\"content_length\"], bins=100, alpha=0.5, label=\"imputed_quality = 0\")\n\nplt.show()\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"content_length\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"content_length\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot:xlabel='content_length', ylabel='Density'&gt;\n\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"rating_diff\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"rating_diff\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot:xlabel='rating_diff', ylabel='Density'&gt;\n\n\n\n\n\n\n\nplt.figure()\n\nplt.hist(df[df[\"imputed_quality\"] == 1][\"word_entropy\"], bins=100, alpha=0.5, label=\"imputed_quality = 1\")\nplt.hist(df[df[\"imputed_quality\"] == 0][\"word_entropy\"], bins=100, alpha=0.5, label=\"imputed_quality = 0\")\n\n#set axes labels\nplt.xlabel(\"word_entropy\")\nplt.ylabel(\"count\")\n\n\nplt.show()\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"word_entropy\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"word_entropy\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot:xlabel='word_entropy', ylabel='Density'&gt;\n\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"space_length_ratio\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"space_length_ratio\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot:xlabel='space_length_ratio', ylabel='Density'&gt;\n\n\n\n\n\n\n# kde for mentions_prof_word\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"mentions_prof_word\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"mentions_prof_word\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot:xlabel='mentions_prof_word', ylabel='Density'&gt;\n\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"contains_pronouns\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"contains_pronouns\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot:xlabel='contains_pronouns', ylabel='Density'&gt;\n\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"average_sentence_length\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"average_sentence_length\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot:xlabel='average_sentence_length', ylabel='Density'&gt;\n\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"ratio_all_cap_words\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"ratio_all_cap_words\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot:xlabel='ratio_all_cap_words', ylabel='Density'&gt;\n\n\n\n\n\n\nsns.kdeplot(df[df[\"imputed_quality\"] == 1][\"lexical_density_estimate\"], fill=True, label=\"imputed_quality = 1\")\nsns.kdeplot(df[df[\"imputed_quality\"] == 0][\"lexical_density_estimate\"], fill=True, label=\"imputed_quality = 0\")\n\n&lt;AxesSubplot: xlabel='lexical_density_estimate', ylabel='Density'&gt;\n\n\n\n\n\n\nplt.figure()\n\ncolor_map = {\n    0: \"blue\",\n    1: \"green\"\n}\n\nalpha_map = {\n    0: 0.1,\n    1: 0.5\n}\n\nax = plt.scatter(df[\"default_mse\"], df[\"content_length\"], c=df[\"imputed_quality\"].map(color_map), alpha=df[\"imputed_quality\"].map(alpha_map))\n\nplt.show()\n\n\n\n\n\nplt.figure()\n\ncolor_map = {\n    0: \"blue\",\n    1: \"green\"\n}\n\nalpha_map = {\n    0: 0.1,\n    1: 0.5\n}\n\nax = plt.scatter(df[\"entropy\"], df[\"average_sentence_length\"], c=df[\"imputed_quality\"].map(color_map), alpha=df[\"imputed_quality\"].map(alpha_map))\n\nplt.show()\n\n\n\n\n\nplt.figure()\n\ncolor_map = {\n    0: \"blue\",\n    1: \"green\"\n}\n\nalpha_map = {\n    0: 0.1,\n    1: 0.5\n}\n\nax = plt.scatter(df[\"default_mse\"], df[\"content_length\"], c=df[\"imputed_quality\"].map(color_map), alpha=df[\"imputed_quality\"].map(alpha_map))\n\nplt.show()\n\n\n\n\n\nplt.figure()\n\ncolor_map = {\n    0: \"blue\",\n    1: \"green\"\n}\n\nalpha_map = {\n    0: 0.1,\n    1: 0.5\n}\n\nplt.hist(df[df[\"wasAuthorized\"] == 1][\"votes\"], bins=5, alpha=0.5, label=\"imputed_quality = 1\")\nplt.hist(df[df[\"wasAuthorized\"] == 0][\"votes\"], bins=5, alpha=0.5, label=\"imputed_quality = 0\")\n\nplt.show()\n\n\n\n\n\n# cor\nimport seaborn as sns\n\n\ncorr_features = [\n    \"default_mse\",\n    \"overly_negative_mse\",\n    # \"overly_positive_mse\",\n    # \"slider_variance\",\n    # \"time_between_course_and_review\",\n\n    # \"knn_avg_distance\",\n    \"content_larger_than_250\",\n    \"content_length\",\n\n    \"rating_diff\",\n\n    # \"space_length_ratio\", #sucks\n    # \"percent_difficult_words\", #sucks\n    # \"ratio_question_marks\", #sucks\n    # \"ratio_exclamation_marks\", #sucks\n\n\n    \"number_count\",\n    \"mentions_prof_word\",\n    \"contains_pronouns\",\n    \"contains_personal_pronouns\",\n    # \"mentions_prof_name\",\n    # \"mentions_grades\", #sucks\n\n    \"ttr\",\n    \"cttr\",\n\n    \"sentiment_scores\",\n    \"neutrality_scores\",\n      \n    # \"polarity\", #sucks\n    # \"consistent_polarity\",\n    \n    \"average_sentence_length\",\n    \"fre_readability\",\n    \"smog_readability\",\n    \"dale_chall_readability\",\n    \"entropy\",\n    \"word_entropy\",\n    \"wasAuthorized\",\n    # \"word_diversity\",\n\n]\n\ncorr_features.append(\"imputed_quality\")\n\nplt.figure(figsize=(15,15))\n\nsns.heatmap(df[corr_features].corr(), annot=True)\n            \nplt.show()\n\n\n\n\n\n# LR\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nseed = 42\n\nX_words = v.transform(df[\"content\"])\nX_words_row_sum = X_words.sum(axis=1)\n\nX = df[corr_features[:-1]]\nX = np.concatenate((X, X_words.toarray()), axis=1)\nX = np.concatenate((X, X_words_row_sum), axis=1)\n\ny = df[\"imputed_quality\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=seed, stratify=y)\n\nlr = LogisticRegression(\n    penalty=\"l2\",\n    max_iter=1e12,\n    class_weight=\"balanced\",\n    random_state=seed,\n)\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\nprint(confusion_matrix(y_test, y_pred))\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.72      0.71      0.72        89\n           1       0.75      0.77      0.76       104\n\n    accuracy                           0.74       193\n   macro avg       0.74      0.74      0.74       193\nweighted avg       0.74      0.74      0.74       193\n\n[[63 26]\n [24 80]]\n\n\nc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n  warnings.warn(\n\n\n\n# Q: what does our model get wrong?\n\ndf_2 = df.copy()\ndf_2[\"pred\"] = lr.predict(X)\n\ndf_2[\"pred_correct\"] = df_2[\"pred\"] == df_2[\"imputed_quality\"]\n\ndf_incorrect = df_2[df_2[\"pred_correct\"] == False][[\"imputed_quality\", \"content\"]]\n\n\nc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n  warnings.warn(\n\n\n\n# RF\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nseed = 42\n\nrf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    random_state=seed,\n    class_weight=\"balanced\",\n    max_features=\"sqrt\",\n)\n\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\nprint(confusion_matrix(y_test, y_pred))\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.69      0.69      0.69        89\n           1       0.73      0.73      0.73       104\n\n    accuracy                           0.71       193\n   macro avg       0.71      0.71      0.71       193\nweighted avg       0.71      0.71      0.71       193\n\n[[61 28]\n [28 76]]\n\n\nc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n  warnings.warn(\n\n\n\n# corr of X\n\ncolumns = corr_features[:-1]\ncolumns.extend([\"WORD_\" + word for word in v.get_feature_names_out()])\ncolumns.append(\"total_word_sum\")\n\nxydf = pd.DataFrame(X, columns = columns)\nxydf_features = xydf.columns\nxydf['imputed_quality'] = y\n\n\n# print(xydf.corr(numeric_only=True))\n\n# get sorted list of columns with highest correlation to imputed_quality\nprint(xydf.corr(numeric_only=True)[\"imputed_quality\"].sort_values(ascending=False).head(20))\nprint(\"=====================================\")\nprint(xydf.corr(numeric_only=True)[\"imputed_quality\"].sort_values(ascending=True).head(5)) # negative correlation\n\n\n# xdf = pd.DataFrame(X, columns = columns)\n# print(xdf.corr(numeric_only=True))\n\nimputed_quality               1.000000\nword_entropy                  0.438223\ncttr                          0.416696\ncontent_length                0.401542\ncontent_larger_than_250       0.363106\nsmog_readability              0.271447\ntotal_word_sum                0.249991\naverage_sentence_length       0.224778\nentropy                       0.210879\noverly_negative_mse           0.178128\nsentiment_scores              0.171686\nnumber_count                  0.150418\nrating_diff                   0.138496\ndefault_mse                   0.136861\ncontains_personal_pronouns    0.122083\ncontains_pronouns             0.114012\nneutrality_scores             0.095530\nWORD_students                 0.090709\nmentions_prof_word            0.090445\ndale_chall_readability        0.089281\nName: imputed_quality, dtype: float64\n=====================================\nttr                -0.293566\nfre_readability    -0.141662\nWORD_easy          -0.047449\nWORD_pretty        -0.046711\nWORD_interesting   -0.025898\nName: imputed_quality, dtype: float64\n\n\n\n# # Multiclass (-1, 0, 1) LR\n\n# def get_mutliclass_quality(row):\n#     if row[\"votes\"] &lt; 0:\n#         return -1\n#     if row[\"votes\"] &gt; 1:\n#         return 1\n    \n#     return 0\n\n# df[\"imputed_multiclass_quality\"] = df.apply(get_mutliclass_quality, axis=1)\n\n\n# df[\"imputed_multiclass_quality\"].value_counts()\n\n\n# # Multiclass LR\n\n# from sklearn.linear_model import LogisticRegression\n\n# X_words = v.transform(df[\"content\"])\n# X_words_row_sum = X_words.sum(axis=1)\n\n# X = df[corr_features[:-1]]\n# X = np.concatenate((X, X_words.toarray()), axis=1)\n# X = np.concatenate((X, X_words_row_sum), axis=1)\n\n# y = df[\"imputed_multiclass_quality\"]\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n\n# lr = LogisticRegression(\n#     penalty=\"l2\",\n#     max_iter=1e10,\n#     # class_weight=\"balanced\",\n# )\n# lr.fit(X_train, y_train)\n\n# y_pred = lr.predict(X_test)\n\n# print(classification_report(y_test, y_pred))\n\n# print(confusion_matrix(y_test, y_pred))\n\n=&gt; Multiclass LR is not helpful\n\n# # corr relative to imputed_multiclass_quality\n\n# columns = corr_features[:-1]\n# # columns.extend([\"word_\" + word for word in v.get_feature_names_out()])\n# # columns.append(\"word_sum\")\n\n# X = df[columns].to_numpy()\n\n# xydf = pd.DataFrame(X, columns = columns)\n# xydf['imputed_multiclass_quality'] = y\n\n# xydf.corr()[\"imputed_multiclass_quality\"].sort_values(ascending=False).head(20)\n\n\n# # try polynomial features\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nX2 = df[corr_features[:-1]].to_numpy()\ny2 = df[\"imputed_quality\"].to_numpy()\n\n\npoly = PolynomialFeatures(\n    interaction_only=True,\n    include_bias=False,\n\n)\n\nX_poly = poly.fit_transform(X2)\n\n# X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.1, stratify=y)\n\n# lr = LogisticRegression(\n#     penalty=\"l2\",\n#     max_iter=1e12,\n#     class_weight=\"balanced\",\n#     random_state=seed,\n# )\n\n# lr.fit(X_train, y_train)\n\n# y_pred = lr.predict(X_test)\n\n# print(classification_report(y_test, y_pred))\n\n\n\n\n\n\n# Corr of poly features\n\n# sns.heatmap(pd.DataFrame(X_poly).corr(), annot=True)\n\npoly_cols = poly.get_feature_names_out(corr_features[:-1])\n\npoly_df = pd.DataFrame(X_poly, columns=poly_cols)\npoly_df[\"imputed_quality\"] = y2\n\nprint(poly_df.corr()[\"imputed_quality\"].sort_values(ascending=False).head(20))\n\nimputed_quality                                    1.000000\nword_entropy                                       0.438223\nentropy word_entropy                               0.435394\ncontent_length ttr                                 0.422257\ncttr word_entropy                                  0.420021\ncontent_larger_than_250 cttr                       0.419266\ncttr entropy                                       0.416896\ncttr                                               0.416696\noverly_negative_mse content_length                 0.413907\ncontent_larger_than_250 content_length             0.409462\ncontent_larger_than_250 smog_readability           0.406417\ncontent_length                                     0.401542\ncontent_length entropy                             0.399604\ncontent_larger_than_250 word_entropy               0.398910\ncontent_length dale_chall_readability              0.397620\ncontent_length neutrality_scores                   0.392154\ncontent_length smog_readability                    0.392014\ncontent_length fre_readability                     0.390966\ncontent_length word_entropy                        0.383515\ncontent_larger_than_250 average_sentence_length    0.382199\nName: imputed_quality, dtype: float64\n\n\n\nprint(poly_df.corr()[\"imputed_quality\"].sort_values(ascending=False).tail(20))\n\ncontains_pronouns wasAuthorized           0.063840\nword_entropy wasAuthorized                0.062105\nmentions_prof_word fre_readability        0.060904\nmentions_prof_word wasAuthorized          0.051723\nmentions_prof_word ttr                    0.048747\ndale_chall_readability wasAuthorized      0.030803\nneutrality_scores wasAuthorized           0.029866\nentropy wasAuthorized                     0.021198\nwasAuthorized                             0.016222\nfre_readability wasAuthorized            -0.013584\nttr wasAuthorized                        -0.022016\nneutrality_scores fre_readability        -0.063622\nfre_readability dale_chall_readability   -0.092611\nttr dale_chall_readability               -0.097755\nfre_readability entropy                  -0.119633\nttr neutrality_scores                    -0.128637\nfre_readability                          -0.141662\nttr entropy                              -0.251689\nttr fre_readability                      -0.269947\nttr                                      -0.293566\nName: imputed_quality, dtype: float64\n\n\n\n# Choose First 20 features and last 5 features\n\npoly_features = poly_df.corr()[\"imputed_quality\"].sort_values(ascending=False).head(20).index.to_list()\npoly_features.extend(poly_df.corr()[\"imputed_quality\"].sort_values(ascending=False).tail(5).index.to_list())\n\n#remove \"imputed_quality\"\n# poly_features.remove(\"imputed_quality\")\n\nprint(poly_features)\n\n\n# Corr plot of poly features\nplt.figure(figsize=(15, 15))\nsns.heatmap(poly_df[poly_features].corr(), annot=True)\n\n\n#remove \"imputed_quality\"\npoly_features.remove(\"imputed_quality\")\n\n['imputed_quality', 'word_entropy', 'entropy word_entropy', 'content_length ttr', 'cttr word_entropy', 'content_larger_than_250 cttr', 'cttr entropy', 'cttr', 'overly_negative_mse content_length', 'content_larger_than_250 content_length', 'content_larger_than_250 smog_readability', 'content_length', 'content_length entropy', 'content_larger_than_250 word_entropy', 'content_length dale_chall_readability', 'content_length neutrality_scores', 'content_length smog_readability', 'content_length fre_readability', 'content_length word_entropy', 'content_larger_than_250 average_sentence_length', 'ttr neutrality_scores', 'fre_readability', 'ttr entropy', 'ttr fre_readability', 'ttr']\n\n\n\n\n\n\n#KDE of best poly features\n\nfeature = poly_features[0]\n\nplt.figure(figsize=(10, 10))\nsns.kdeplot(\n    data=poly_df, \n    x=feature, \n    hue=\"imputed_quality\", \n    fill=True, \n    common_norm=False, \n    alpha=.5, \n    linewidth=0,\n    bw_adjust=2,\n    )\n\n&lt;AxesSubplot: xlabel='word_entropy', ylabel='Density'&gt;\n\n\n\n\n\n\n\nfeature = poly_features[2] + \"x\" + poly_features[0]\n\npoly_df[feature] = poly_df[poly_features[2]] * poly_df[poly_features[0]]\n\nplt.figure(figsize=(10, 10))\nsns.kdeplot(\n    data=poly_df, \n    x=feature, \n    hue=\"imputed_quality\", \n    fill=True, \n    common_norm=False, \n    alpha=.5, \n    linewidth=0,\n    bw_adjust=2,\n    )\n\nprint(poly_df[[feature, \"imputed_quality\"]].corr())\n\n                                 content_length ttrxword_entropy  \\\ncontent_length ttrxword_entropy                            1.000   \nimputed_quality                                            0.404   \n\n                                 imputed_quality  \ncontent_length ttrxword_entropy            0.404  \nimputed_quality                            1.000  \n\n\n\n\n\n\n# LR with poly features\n\nX3 = poly_df[poly_features].to_numpy()\ny3 = poly_df[\"imputed_quality\"].to_numpy()\n\n\nX_train, X_test, y_train, y_test = train_test_split(X3, y3, test_size=0.1, stratify=y)\n\nlr = LogisticRegression(\n    penalty=\"l2\",\n    max_iter=1e12,\n    class_weight=\"balanced\",\n    random_state=seed,\n)\n\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.61      0.75      0.67        89\n           1       0.73      0.59      0.65       104\n\n    accuracy                           0.66       193\n   macro avg       0.67      0.67      0.66       193\nweighted avg       0.68      0.66      0.66       193\n\n\n\n\n# vectorize using all the numeric features then project into 2D and plot\n\nfrom sklearn.manifold import TSNE\n\nX4 = df[corr_features[:-1]].to_numpy()\ny4 = df[\"imputed_quality\"].to_numpy()\n\nX_embedded = TSNE(n_components=2).fit_transform(X4)\n\nplt.figure(figsize=(10, 10))\n\nsns.scatterplot(\n    x=X_embedded[:, 0],\n    y=X_embedded[:, 1],\n    hue=y4,\n    alpha=0.5,\n    palette=\"deep\",\n    s=100,\n    linewidth=0,\n)\n\nplt.show()\n\nc:\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\nc:\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n\n\n\n\n\n\n# vectorize using all the numeric features then project into 2D and plot\n\nfrom sklearn.manifold import TSNE\n\nX4 = xydf[xydf_features].to_numpy()\ny4 = xydf[\"imputed_quality\"].to_numpy()\n\npred = lr.predict(X4)\npred_correct = (pred == y4)*1\n\nX_embedded = TSNE(n_components=2).fit_transform(X4)\n\nplt.figure(figsize=(10, 10))\n\nsns.scatterplot(\n    x=X_embedded[:, 0],\n    y=X_embedded[:, 1],\n    hue=y4,\n    alpha=0.5,\n    palette=\"deep\",\n    s=100,\n    linewidth=0,\n)\n\nplt.show()\n\nc:\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\nc:\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n\n\n\n\n\n\n# vectorize using all the numeric features then project into 2D and plot\n\nfrom sklearn.manifold import TSNE\n\nX4 = xydf[xydf_features].to_numpy()\ny4 = xydf[\"imputed_quality\"].to_numpy()\n\npred = lr.predict(X4)\npred_correct = (pred == y4)*1\n\nX_embedded = TSNE(n_components=2).fit_transform(X4)\n\nc:\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\nc:\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n\n\n\nplt.figure(figsize=(10, 10))\npred_incorrect = (pred != y4)*1\n\n\nedge_color_map = {0: \"red\", 1: \"blue\"}\ncolor_map = {0: \"grey\", 1: \"orange\"}\n\n\nsns.scatterplot(\n    x=X_embedded[:, 0],\n    y=X_embedded[:, 1],\n    hue=pred_incorrect,\n    edgecolor=list(map(lambda x: edge_color_map[x], pred)),\n    linewidth=1,\n    palette={0: \"lightgrey\", 1: \"orange\"},\n    alpha=(0.05+0.5*pred_incorrect),\n    s=100,\n)\n\nplt.show()\n\n\n\n\nNo discernible pattern in the TSNE embedding space for where incorrect predictions are made\n\n# vectorize using all the numeric features then project into 3D and plot\n\nfrom sklearn.manifold import TSNE\n\nX4 = df[corr_features[:-1]].to_numpy()\ny4 = df[\"imputed_quality\"].to_numpy()\n\nX_embedded = TSNE(n_components=3).fit_transform(X4)\n\nplt.figure(figsize=(10, 10))\n\nax = plt.axes(projection='3d')\n\nax.scatter3D(\n    X_embedded[:, 0],\n    X_embedded[:, 1],\n    X_embedded[:, 2],\n    c=y4,\n    alpha=0.5,\n    s=100,\n    linewidth=0,\n)\n\nplt.show()\n\nc:\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\nc:\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn("
  },
  {
    "objectID": "posts/Project/projectReport.html",
    "href": "posts/Project/projectReport.html",
    "title": "Classifying MiddCourses Reviews",
    "section": "",
    "text": "We developed review quality classifiers based on text and metadata features from course reviews on MiddCourses, a student course evaluation site for Middlebury College. Using human annotation, split between 3 annotators, we determined the quality levels of reviews from a dataset of roughly 1,900 reviews. We fixed a annotation threshold such that our resulting binary labels (high and low review quality) were roughly equal. We then built and tested over 50 features and selected the top features based on their correlation with our quality labels and on the level of difference in the kernel density distribution of the feature when faceted by our quality label. We trained a logistic regression classifier on these engineered features and achieved an f1 score of about 0.75 for each class. We also trained XLNet, a transformer based, model on the text content of our reviews, achieving an f1 score of 0.65 for low-quality reviews and 0.72 for high-quality reviews.\nOur source code is available at https://github.com/Nicholas-Sliter/csci0451-final-project."
  },
  {
    "objectID": "posts/Project/projectReport.html#related-work",
    "href": "posts/Project/projectReport.html#related-work",
    "title": "Classifying MiddCourses Reviews",
    "section": "Related Work",
    "text": "Related Work\nWe started our research by looking at existing work on review helpfulness prediction. We found that there are two main approaches to this problem:\n\nMetadata-based: uses metadata and computed features to predict helpfulness (Du et al. 2019; Singh et al. 2017; Mauro, Ardissono, and Petrone 2021; Liu et al. 2007)\nText-based: uses NLP-based methods or deep learning on review text to predict helpfulness (Mohawesh et al. 2021; Salminen et al. 2022; Lubis, Rosmansyah, and Supangkat 2017)"
  },
  {
    "objectID": "posts/Project/projectReport.html#data",
    "href": "posts/Project/projectReport.html#data",
    "title": "Classifying MiddCourses Reviews",
    "section": "Data",
    "text": "Data\nWe used fully anonymous (user-unaware) data from MiddCourses of reviews submitted prior to 2023-04-21 18:09:01.856+00. This data was provided to us by the MiddCourses team. The SQL query used to gather the data is available on GitHub in the code repository.\nEach row represents a review of a course-instructor pair.\nA full datasheet is available at https://github.com/Nicholas-Sliter/middcourses2#review-structure.\n\nLimitations\nThe review data is UGC (user-generated content) and therefore has the following limitations:\n\nResponse Bias: The data is biased towards users who are more likely to leave reviews. This means that the data is not representative of the entire population of Middlebury students. This is a common problem with UGC and is difficult to mitigate.\nExtremity Bias: As review data, MiddCourses reviews exhibit extremity bias. This means that reviews are more likely to be either very positive or very negative. This is a well-known problem with review data and is difficult to mitigate.\nAcquiescence Bias: Reviewers may feel pressured to leave a positive review for a course or instructor. This is especially true for courses and instructors that are well-liked.\n\nReviewers may also feel compelled to leave reviews due to the incentive structure of the site. These reviews may not exhibit an accurate representation of the course or instructor. Indeed, it is these reviews that we want to identify and de-rank."
  },
  {
    "objectID": "posts/Project/projectReport.html#approach",
    "href": "posts/Project/projectReport.html#approach",
    "title": "Classifying MiddCourses Reviews",
    "section": "Approach",
    "text": "Approach\nWe approached the problem by conducting experiments with feature engineering and NLP methods to identify a set of features that would effectively classify reviews based on quality. On top of that, we trained our data on a deep language model (XLNet) to compare those results to the results of our own feature engineering. Our objective was to design a classification model that most accurately predicted review quality based on the input data.\n\n\n\nTo begin, we performed triple-annotation on the entire dataset using an annotation tool (midd.courses/annotate) and transformed the annotations into classification labels based on a certain threshold. Since there were no inherent labels for review quality, we relied on annotations as the ground truth. Recognizing the subjectivity of this process, we each individually annotated the data to mitigate bias.\n\nExample of a review on the annotation tool\n\nIn order to ensure that our annotations were consistent, we created an annotation rubric. The rubric gave us a rough idea of whether to give the annotation a thumbs up (+1), thumbs down (-1), or leave it blank (0):\n\n\n\n\n\n\n\n\nGreat review (+1)\nOk review (+0)\nBad review (-1)\n\n\n\n\nText consistent with sliders\nText mostly consistent with sliders\nText is not consistent with sliders\n\n\nText is well written\nText is well written\nText is difficult to read\n\n\nGives a balanced picture of the course\nGives a limited perspective on the course (e.g. only negative)\nGives a very limited perspective on the course (e.g. hyperbolically negative)\n\n\nText offers valuable information not found in the sliders and other questions\nText does not offer much information about the course not found in the sliders and other questions\nText is irrelevant to the course or contains irrelevant meta information (e.g. personal information, spam, padding to make character count, etc)\n\n\n\nThis rubric gave us a consistent guideline by which to score the reviews. Once the annotations were complete, each review received a vote score which was the sum of the annotation values. We then decided on the threshold of 0 to create our target vector \\(y\\). Any review below this threshold received an imputed quality of 0 (indicating lower quality) and any review above the threshold received an imputed quality of 1.\n\nMetadata-based approach (Feature Engineering & Logistic Regression):\nFor the initial model, we engineered features using both the textual content of the reviews and the non-textual metadata associated with each review. This process entailed brainstorming and developing the features, evaluating their correlation with the imputed_quality (our target vector), and assessing the intercorrelation among the features to mitigate multicollinearity. We then chose the highest performing features and ran them through a linear regression model.\nThe list below shows all of the features we selected for our classification task.\nMetadata-based:\n\n\n\n\n\n\n\n\nFeature\nDescription\nQual./Quant.\n\n\n\n\ndefault_mse\nMean squared error between default slider values and the slider values of the review\nquant.\n\n\noverly_negative_mse\nMean squared error between overly negative slider values and the slider values of the review\nquant.\n\n\nrating_diff\nDifference between the default value of the rating slider and the value of the rating slider for the review\nquant.\n\n\nwasAuthorized\nUser had authorization to view other reviews at the time they submitted their review\nqual.\n\n\n\nText-based:\n\n\n\n\n\n\n\n\nFeature\nDescription\nQual./Quant.\n\n\n\n\ndefault_mse\nMean squared error between default slider values and the slider values of the review\nquant.\n\n\noverly_negative_mse\nMean squared error between overly negative slider values and the slider values of the review\nquant.\n\n\nrating_diff\nDifference between the default value of the rating slider and the value of the rating slider for the review\nquant.\n\n\ncontent_larger_than_250\nText contains more than 250 characters\nqual.\n\n\ncontent_length\nNumber of characters in text (normalized)\nquant.\n\n\nnumber_count\nNumber of numbers in text\nquant.\n\n\nmentions_prof_word\nText mentions any word related to “professor”\nqual.\n\n\ncontains_pronouns\nText contains pronouns\nqual.\n\n\ncontains_personal_pronouns\nText contains personal pronouns\nqual.\n\n\naverage_sentence_length\nAverage length of sentence in text\nquan.\n\n\nfre_readability\nFlesch Reading Ease, a formula that measures the text readability (from 0-100) based on the average length of your sentences (measured by the number of words) and the average number of syllables per word\nquant.\n\n\nsmog_readability\nSMOG index, a readability formula that assesses the “grade-level” of text based on the number of polysyllabic words\nquant.\n\n\ndale_chall_readability\nDale-Chall readability formula, a method used to determine the approximate “grade-level” of a text based on sentence length and the number of “hard” words\nquant.\n\n\nentropy\nShannon entropy based on the frequency of each character in the text\nquant.\n\n\nword_entropy\nShannon entropy based on the frequency of each word in the text\nquant.\n\n\nsentiment_scores\nSentiment scores calculated using vaderSentiment’s SentimentIntensityAnalyzer\nquant.\n\n\nneutrality_scores\nNeutrality scores calculated using vaderSentiment’s SentimentIntensityAnalyzer\nquant.\n\n\n\nFinally, we also included as a feature a vectorized version of the text. The vectorizer applies TF-IDF (Term Frequency-Inverse Document Frequency) weighting to prioritize important words while filtering out common and less informative ones, using specified parameters such as maximum features, maximum document frequency, minimum document frequency, and a set of English stop words.\nIn order to select the best features, we looked at the coorelation between features and imputed_quality and feature intercoorelation. Furthermore, we looked at density plots to discover quantitative features that caused different distributions for reviews with differing imputed_quality.\n\nDensity plots for two features of varying quality\n\n\nCorrelation matrix of our selected features\n\n\n\nText-based approach (XLNet):\nIn an attempt to achieve better results, we also experimented with using a deep model to classify reviews. While exploring the possible options for deep NLP, we came across multiple sequence-in architectures such as Recurrent Neural Networks (RNNs), Long-Short Term Memory (LSTM) and Gated Recurrent Units (GRU). Eventually we decided to implement the Transformer architecture, one of the most prominent recent advancements in deep learning.\nIn a 2017 paper titled “Attention is All You Need,” the the Transformer architecture was proposed by a team led by Ashish Vaswami (Vaswani et al. 2017). It has since become the go-to method for a wide range of NLP tasks and it’s probably most well known for its use in GPT-3.\nUnlike previous NLP approaches that often relied on encoder-decoder recurrent neural networks (RNNs) for sequence modeling, the Transformer architecture introduced an attention mechanism between the encoder and decoder (Cristina 2023b). Attention is a method that allows the neural network to focus on only the most important parts of the input sequence. Furthermore, it eliminates the issue of diminishing gradients found when using RNNs. In other words, accuracy does not drop when the number of inputs in the sequence increases.\n\nTypical sequence-in sequence-out RNN with encoder and decoder from “Sequence to Sequence Learning with Neural Networks”\n\n\nEncoder decoder with attention from “Attention in Psychology, Neuroscience, and Machine Learning”\n\nBy using self-attention mechanisms, the Transformer architecture avoids the sequential nature of RNNs, making it highly parallelizable and quickly optimizable (through backpropagation). It therefore trains very efficiently on parallel processors like GPUs (Cristina 2023b).\n\nFull transformer model from “Attention is All You Need”\n\nOur specific model utilizes XLNet. XLNet is an open-source Transformer architecture that has been pre trained using permutation-based autoregressive training to address limitations of the traditional autoregressive models, mainly the lack of bidirectional context (Yang et al. 2020; Cristina 2023a). This modification enables XLNet to better capture bidirectional context, leading to improved performance on several NLP tasks.\nWe accessed XLNet through SimpleTransformers based on the Transformers library by HuggingFace. This allowed us to quickly implement the Transformer architecture and then tune the parameters to figure out the best outcomes. We created the model to take in text as a sequence of words (which are embedded) and then output either 0 or 1 as the predicted imputed_quality. We also trained only the final layer of the model.\nSimple transformers documentation: https://simpletransformers.ai/docs/installation/\nFurther reading on Transformer architecture: - https://machinelearningmastery.com/a-tour-of-attention-based-architectures/ - https://machinelearningmastery.com/the-transformer-model/"
  },
  {
    "objectID": "posts/Project/projectReport.html#nicholas",
    "href": "posts/Project/projectReport.html#nicholas",
    "title": "Classifying MiddCourses Reviews",
    "section": "Nicholas",
    "text": "Nicholas\nI worked on much of the feature engineering for the metadata-based model. I also worked on the data collection and annotation, building the annotation tool on MiddCourses and providing the data. For the final report, I worked on the abstract, introduction, values-statement, and part of the methods sections."
  },
  {
    "objectID": "posts/Project/projectReport.html#paul",
    "href": "posts/Project/projectReport.html#paul",
    "title": "Classifying MiddCourses Reviews",
    "section": "Paul",
    "text": "Paul\nI worked on annotating the reviews on MiddCourses website and finding ways to measure lexical richness using a Python module called LexicalRichness. For the final report I worked on the results in addition to parts of the conclusion."
  },
  {
    "objectID": "posts/Project/projectReport.html#aidan",
    "href": "posts/Project/projectReport.html#aidan",
    "title": "Classifying MiddCourses Reviews",
    "section": "Aidan",
    "text": "Aidan\nI worked on researching deep NLP methods and implementing and fine-tuning the XLnet. I also annotated the reviews and helped out a little with the feature engineering. For the final report I wrote both the metadata and text-based part of the approach section."
  }
]