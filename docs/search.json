[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/TimnitGebru/index.html",
    "href": "posts/TimnitGebru/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "About Dr. Timnit Gebru\nDr. Timnit Gebru is a computer scientist who specializes machine learning and artificial intelligence. She cofounded a research organization alongside Rediet Abebe known as Black in AI, which seeks to speak out about the lack of diversity in the field of artificial intelligence. Many of her works address racial discrimination and gender bias. Dr. Gebru coauthored a paper wher research showed that facial recognition was less accurate in identifying people of color and women. This would mean that there would be discrimination against these groups. This led to another research by other computer scientists and found that in facial recognition, black women were 35% less likely to be recognized than white men, further proving Dr. Gebru’s argument. Dr. Gebru is a recognized voice in artificial intelligence because she continues to uncover the ethical issues of the field even if it negatively impacts her. For example, she and five others did an research and put their findings on a paper called “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” during her time at Google. Despite the opposition from the higer-ups, Gebru insisted that the paper be published, which led to her departure from the company. Regardless of these things, she continues to move forward with the effort the address social injustices in the field of AI through research and starting organizations like Black in AI and the Distributed Artificial Intelligence Research Institute.\n\n\nTutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision\nIn Dr. Gebru’s talk, she speaks about how the field of computer vision is negatively affecting certain groups of people. She stated that computer vision is being utilized for things like the hiring process, surveillance, and policing. Although computer vision has its pros, there are also cons. For example, Dr. Gebru points out the use of face recognition in Maryland, where they used it to identify the Annapolis Capital Gazette shooter, and while this was a benefit, the police started to use it on photos to identify individuals at the Freddit Gray protests. In addition to surveillance, there are racial issues of computer vision. She noted that there were high disparity between black women and white men. Facial recognition was accurate when identifying white men but were much less accurate when identifying black women. Some companies believed that they did not have enough data to correctly identify certain groups of people such as people of the transgender community. As a result, they used videos of Transgender Youtubers without permission to enhance their data for facial/gender recognition.\nComputer vision algorithms can be useful, but it can cause a lot more harm than one might think.\n\nQuestion for Dr. Gebru\nWhat can companies do to prevent algorithmic biases in computer vision?\n\n\n\nSummary of Dr. Gebru’s Talk\nThe main focus on Dr. Gebru’s talk on Monday was about AGI (artificial general intelligence). In her talk, she briefly goes over the first wave of eugenics, movement that sought to improve the human population through sterilization, selective breeding, etc. and also the second wave of eugenics. Dr. Gebru argues that the current growth of AGI is correlated to the second wave of eugenics. She mentions that both the first and the second wave of eugenics (mostly the latter) are issues that are starting to surface as the field of AGI grows because these AGI models select the traits of humans that it deems helpful to the human population.\nDr. Gebru also explains that AGI is being used to create a utopian society. She describes that basic human needs will be met and the problems of the world such as poverty will be solved through the development in AGI. However, she argues that this utopian society that is promised through AGI will only benefit the rich. Another reason she links the development in AGI to second wave eugenics is due to its transhumanism ideology. Dr. Gebru notes that the field of AGI is being geared towards developing a society where humans and technology are integrated. Transhumanists believe that the integration of human and technology will improve society as a whole. Dr. Gebru raises concerns for this ideology and that only the privileged will be able to improve their living standards while others are left behind.\nI agree with the arguments of Dr. Gebru. Although I am fairly new to the ideas that Dr. Gebru has brought up in the talk, I believe I can agree with the fact that TESCREAL is ethically wrong even if they believe they are doing good for the future of the human population. AI may not seem like it is related to the second wave eugenics, but I can agree with Dr. Gebru with the fact that in the near future, their relation will become clearer.\n\n\nMy Thoughts\nAttending the talk by Dr. Gebru was a great experience and the topic was very interesting. Before attending the talk, I had never heard any of the TESCREAL terms, but after attending, it helped me aware of what is going on in the field of artificial intelligence and that it is being pushed in this direction. I became interested and researched on my own time about how the second wave of eugenics is related to AGI. Something that I was sort of aware was transhumanism; the integration of man and machine is something that I have always disagreed with and after the talk I became certain of my opinion."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\nPenguins blog post\n\n\n\n\n\n\nMay 21, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression\n\n\n\n\n\nBlog Post for Logistic Regression\n\n\n\n\n\n\nMay 13, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nTimnit Gebru\n\n\n\n\n\nBlog Post about Dr. Gebru’s Talk\n\n\n\n\n\n\nApr 20, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nPerceptron\n\n\n\n\n\nPerceptron Blog Post\n\n\n\n\n\n\nMar 10, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blogg"
  },
  {
    "objectID": "posts/Perceptron/perceptronNB.html",
    "href": "posts/Perceptron/perceptronNB.html",
    "title": "Perceptron",
    "section": "",
    "text": "Here is the link to the source code: https://github.com/pchoi7/pchoi7.github.io/blob/main/posts/Perceptron/perceptron.py\n\nfrom perceptron import Perceptron\n\n\nPerceptron Implementation\nIn order to perform the perceptron, I created a fit function. In this fit function, I initiate a feature matrix \\(\\tilde{X}\\) of 1’s. The weight vector is given a random number and a for loop is made to perform the perceptron update. In this for loop, the score is appended into the history and the weight is updated using the equation below:\n\\(\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_{i}\\langle \\tilde{w}^{(t)},\\tilde{x}_{i}\\rangle &lt; 0)\\tilde{y}_{i}\\tilde{x}_{i}\\)\n\n\nExperiment (Linearly Separable)\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nprint(\"The accuracy of the perceptron is \" + str(p.score(X, y)))\nprint(\"Score evolution over time is \" + str(p.history[-10:]))\n\nThe accuracy of the perceptron is 1.0\nScore evolution over time is [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\n\n\nExperiment (Non-linearly Separable)\nFor this part of the experiment, the graph was set up so that it would be impossible to be linearly separable. We can see that the data cannot be separated linearly because some of the purple points overlap the yellow data. The accuracy for this data set will never reach 100%.\n\nnp.random.seed(12345)\n\np_2 = Perceptron()\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-0.2, -0.2), (1.5, 1.5)])\n\nfig_2 = plt.scatter(X_2[:,0], X_2[:,1], c = y)\nfig_2 = draw_line(p.w, -2, 2)\n\nxlab_2 = plt.xlabel(\"Feature 1\")\nylab_2 = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig_2 = plt.plot(p_2.history)\nxlab_2 = plt.xlabel(\"Iteration\")\nylab_2 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np_2 = Perceptron()\np_2.fit(X_2, y_2, max_steps = 1000)\n\nprint(\"The accuracy of the perceptron is \" + str(p_2.score(X_2, y_2)))\nprint(\"Score evolution over time is \" + str(p_2.history[-10:]))\n\nThe accuracy of the perceptron is 0.88\nScore evolution over time is [0.84, 0.84, 0.56, 0.71, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88]\n\n\nWe can see that with this data set, the accuracy may fall well below 50-60% for a few iterations. Overall, the perceptron algorithm is pretty accurate (88%).\n\n\nExperiment (Multi-dimensional)\n\nnp.random.seed(12345)\n\np_3 = Perceptron()\n\np_features = 10\nX_3, y_3 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.8, -1.8), (1.8, 1.8)])\n\np_3.fit(X_3, y_3, max_steps = 1000)\n\nfig_3 = plt.plot(p_3.history)\nxlab_3 = plt.xlabel(\"Iteration\")\nylab_3 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nprint(\"The accuracy of the perceptron is \" + str(p_3.score(X_3, y_3)))\nprint(\"Score evolution over time is \" + str(p_3.history[-10:]))\n\nThe accuracy of the perceptron is 1.0\nScore evolution over time is [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nThis data set of 9 dimensions is linearly separable because it reaches 1.0 accuracy.\n\n\nRuntime Complexity\nFor a single iteration of the perceptron algorithm update, the runtime is O(p). The operations that affect the equation \\(\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_{i}\\langle \\tilde{w}^{(t)},\\tilde{x}_{i}\\rangle &lt; 0)\\tilde{y}_{i}\\tilde{x}_{i}\\) is the dot product, multiplication, then addition. Performing the dot product is O(p) + O(p), while performing multiplication and addition is O(1) for both. Therefore, the runtime for equation 1 is O(p)."
  },
  {
    "objectID": "posts/Logistic-Regression/LR.html",
    "href": "posts/Logistic-Regression/LR.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "URL to source code: code\n\nLogistic Regression Implementation\nIn order to implement the \\(\\textit{fit}\\) function, I used the equation for the gradient as follows:\n\\(\\nabla L(w) = \\frac{1}{n} \\sum_{i = 1}^{n}\\nabla \\ell (f_{w}(x_{i}), y_{i})\\)\nThis equation is the gradient for empirical risk for logistic regression. Using this equation, we can substitute it into \\(w^{(t +1)} \\leftarrow w^{(t)} - \\alpha \\nabla L(w^{(t)})\\) and choose our value of \\(\\alpha\\) until it converges.\nFor stochastic gradient descent, a similar equation to the \\(\\textit{fit}\\) function was used:\n\\(\\nabla_{S}L(w) = \\frac{1}{|S|} \\sum_{i \\in S}\\nabla \\ell (f_{w}(x_{i}), y_{i})\\)\nwhere \\(S \\subseteq [n] = \\{1,...,n\\}\\) and is called the batch size.\n\nfrom logReg import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nEvolution of Loss Function for Gradient Descent and Stochastic Gradient Descent\n\n# GRADIENT\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\n\n# STOCHASTIC GRADIENT\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\n\nExperiment 1 (Large Learning rate \\(\\alpha\\))\nIn this experiment, I changed the values of the learning rate \\(\\alpha\\) to see which values fails to converge.\n\nalpha_list = [0.1, 10, 100]  # list of learning rates to test\n\nfor i in range(len(alpha_list)):\n    \n    if i == 0:\n        \n        LR = LogisticRegression()\n        LR.fit(X, y, alpha = alpha_list[i], max_epochs = 75)\n    \n        loss = len(LR.loss_history)\n        plt.plot(np.arange(loss) + 1, LR.loss_history, label = r\"$\\alpha$ = 0.1\")\n    \n    else:\n           \n        LR = LogisticRegression()\n        LR.fit(X, y, alpha = alpha_list[i], max_epochs = 75)\n    \n        loss = len(LR.loss_history)\n        plt.plot(np.arange(loss) + 1, LR.loss_history, label = r\"$\\alpha$ = %d\"%alpha_list[i])\n    \n\nplt.legend()\nplt.xlabel('Number of iterations')\nplt.ylabel('Loss')\nplt.show()\n\n\n\n\nThe loss of a learning rate too high will not converge. In this case, I’ve used a learning rate of \\(\\alpha = 100\\), and the graph shows that for this value the loss does not converge.\n\n\nExperiment 2 (Changing Batch Size)\nIn this experiment, I am changing the batch size to see how it influences the convergence of the algorithm. This experiment uses the fit_stochastic function.\n\n#LR = LogisticRegression()\n\nbatch_list = [10, 50, 100, 1000]  # list of batch sizes to test\n\nfor i in range(len(batch_list)):\n    \n    LR = LogisticRegression()\n    LR.fit_stochastic(X, y, alpha = 0.1, batch_size = batch_list[i], max_epochs = 100)\n    \n    loss = len(LR.loss_history)\n    plt.plot(np.arange(loss) + 1, LR.loss_history, label = \"batch size = %d\"%batch_list[i])\n    \n    \nplt.legend()\nplt.xlabel('Number of iterations')\nplt.ylabel('Loss')\nplt.show()\n\n\n\n\nAs shown in this graph, when the batch size is smaller, the algorithm converges faster."
  },
  {
    "objectID": "posts/Penguins/penguins.html",
    "href": "posts/Penguins/penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer penguins data used in this blog post was collected by Dr. Kristen Gorman and the Palmer Station, which is a member of Long Term Ecological Research Network. The data contains physical measurements of three penguin species: Adelie, Chinstrap, and Gentoo.\n\nData Preparation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import matplotlib as plt\nfrom matplotlib.patches import Patch\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n27\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN46A1\nYes\n11/29/07\n44.5\n14.3\n216.0\n4100.0\nNaN\n7.96621\n-25.69327\nNaN\n\n\n1\nPAL0708\n22\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN41A2\nYes\n11/27/07\n45.1\n14.5\n215.0\n5000.0\nFEMALE\n7.63220\n-25.46569\nNaN\n\n\n2\nPAL0910\n124\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN67A2\nYes\n11/16/09\n41.4\n18.5\n202.0\n3875.0\nMALE\n9.59462\n-25.42621\nNaN\n\n\n3\nPAL0910\n146\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN82A2\nYes\n11/16/09\n39.0\n18.7\n185.0\n3650.0\nMALE\n9.22033\n-26.03442\nNaN\n\n\n4\nPAL0708\n24\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A2\nNo\n11/28/07\n50.6\n19.4\n193.0\n3800.0\nMALE\n9.28153\n-24.97134\nNaN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n1\n45.1\n14.5\n215.0\n5000.0\n7.63220\n-25.46569\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n2\n41.4\n18.5\n202.0\n3875.0\n9.59462\n-25.42621\n0\n0\n1\n1\n0\n1\n0\n1\n\n\n3\n39.0\n18.7\n185.0\n3650.0\n9.22033\n-26.03442\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n4\n50.6\n19.4\n193.0\n3800.0\n9.28153\n-24.97134\n0\n1\n0\n1\n1\n0\n0\n1\n\n\n5\n33.1\n16.1\n178.0\n2900.0\n9.04218\n-26.15775\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n269\n41.1\n17.5\n190.0\n3900.0\n8.94365\n-26.06943\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n270\n45.4\n14.6\n211.0\n4800.0\n8.24515\n-25.46782\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n271\n36.2\n17.2\n187.0\n3150.0\n9.04296\n-26.19444\n0\n0\n1\n1\n1\n0\n1\n0\n\n\n272\n50.0\n15.9\n224.0\n5350.0\n8.20042\n-26.39677\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n273\n48.2\n14.3\n210.0\n4600.0\n7.68870\n-25.50811\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nExplore (Tables and Plots)\n\n# Show species by island \ntrain.groupby(['Species', 'Island'])['Region'].agg('count')\n\nSpecies                                    Island   \nAdelie Penguin (Pygoscelis adeliae)        Biscoe        35\n                                           Dream         41\n                                           Torgersen     42\nChinstrap penguin (Pygoscelis antarctica)  Dream         56\nGentoo penguin (Pygoscelis papua)          Biscoe       101\nName: Region, dtype: int64\n\n\nThis table shows the penguin species and the islands that they were found in. The results show that the Adelie penguins were found in all three islands while the Chinstrap penguins and the Gentoo penguins were found in Dream Island and Biscoe Island, respectively. This may be a good feature to train our model with because we can eliminate penguins depending on which island they were found in. For example, if a penguin were to be found in Torgersen Island, then there is a 100% chance it would be an Adelie penguin based on the table above. If a penguin were to be found in Biscoe Island, then it could either be an Adelie penguin or a Gentoo penguin but not a Chinstrap penguin.\n\n# Shown species by average culmen length, culmen depth, flipper length, and body mass\ntrain.groupby([\"Species\", \"Sex\"])[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\",\n                                  \"Body Mass (g)\"]].mean()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n37.100000\n17.645614\n187.719298\n3337.280702\n\n\nMALE\n40.458182\n19.116364\n192.690909\n4020.454545\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n46.424138\n17.641379\n191.551724\n3514.655172\n\n\nMALE\n51.185185\n19.303704\n199.666667\n3936.111111\n\n\nGentoo penguin (Pygoscelis papua)\n.\n44.500000\n15.700000\n217.000000\n4875.000000\n\n\nFEMALE\n45.600000\n14.242857\n212.928571\n4677.976190\n\n\nMALE\n49.592593\n15.687037\n221.462963\n5502.314815\n\n\n\n\n\n\n\nThis table organizes the average culmen length, culmen depth, flipper length, and body mass by penguin species. The results indicate that the penguin species with the highest average culmen length and depth by sex is the Chinstrap penguin. This can be a useful feature to train the model with because we can label the penguin with the greatest culmen length and depth as the Chinstrap penguin followed by the Gentoo penguin.\n\n# Create plot \nsns.set_theme()\n\nsns.relplot(data = train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n    title=\"Culmen Length vs. Body Mass\")\n\n#sns.relplot(data = train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n#    title=\"Culmen Depth vs. Body Mass\")\n\n#sns.relplot(data = train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\").set(\n#    title=\"Flipper Length vs. Body Mass\")\n\n\n\n\nIn this experiment, I plotted body mass against culmen length for each penguin species. Based on the plot, we can see that Adelie penguins tend to have the smallest body mass as well as the smallest culmen length. Gentoo penguins generally have the same culmen length as Chinstrap penguins but Gentoo penguins are larger in terms of body mass.\n\n\nModel (Choosing Features)\nHere, I use the code provided in the blog post guide to help me select the most useful features. I am using logistic regression, decision tree classifier, and random forest classifier to see which features result in a 1.0 accuracy.\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nLR = LogisticRegression()\nDTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier()\n\n\nLogistic Regression\n\n# Logistic Regression\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    LR.fit(X_train[cols], y_train)\n    print(LR.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.96484375\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.92578125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n0.921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.80078125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.7890625\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.6640625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.95703125\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n0.91796875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.796875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.78515625\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.7421875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n0.859375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.8828125\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.85546875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.78515625\n\n\n\n\nDecision Tree Classifier\n\n# Decision Tree Classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    DTC.fit(X_train[cols], y_train)\n    print(DTC.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.9921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.9765625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.9921875\n\n\n\n\nRandom Forest Classifier\n\n# Random Forest Classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    RFC.fit(X_train[cols], y_train)\n    print(RFC.score(X_train[cols], y_train))\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.9921875\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.9765625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.98046875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n1.0\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.9921875\n\n\nI will be using these features of the decision tree classifier: ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Body Mass (g)’ because they give us in 1.0 accuracy. Below is the proof that the score of the decision tree classifier using these features results in 1.0.\n\n# Accuracy using Decision Tree Classifier on features 'cols'\ncols = ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\nDTC.fit(X_train[cols], y_train)\nprint(\"Accuracy: \" + str(DTC.score(X_train[cols], y_train)))\n\nAccuracy: 1.0\n\n\n\n\n\nEvaluate and Testing\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\ncols = ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n\nDTC_score = DTC.score(X_test[cols], y_test)\nprint(\"Score on test data using Decision Tree Classifier: \" + str(DTC_score))\n\nScore on test data using Decision Tree Classifier: 0.8970588235294118\n\n\n\n# Helper function for plotting (from blog post guide)\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n# Plot Culmen Length vs. Body Mass\nDTC.fit(X_test[cols], y_test)\n\nplot_regions(DTC, X_test[cols], y_test)"
  },
  {
    "objectID": "posts/TimnitGebru/TG_final.html",
    "href": "posts/TimnitGebru/TG_final.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "About Dr. Timnit Gebru\nDr. Timnit Gebru is a computer scientist who specializes machine learning and artificial intelligence. She cofounded a research organization alongside Rediet Abebe known as Black in AI, which seeks to speak out about the lack of diversity in the field of artificial intelligence. Many of her works address racial discrimination and gender bias. Dr. Gebru coauthored a paper wher research showed that facial recognition was less accurate in identifying people of color and women. This would mean that there would be discrimination against these groups. This led to another research by other computer scientists and found that in facial recognition, black women were 35% less likely to be recognized than white men, further proving Dr. Gebru’s argument. Dr. Gebru is a recognized voice in artificial intelligence because she continues to uncover the ethical issues of the field even if it negatively impacts her. For example, she and five others did an research and put their findings on a paper called “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” during her time at Google. Despite the opposition from the higer-ups, Gebru insisted that the paper be published, which led to her departure from the company. Regardless of these things, she continues to move forward with the effort the address social injustices in the field of AI through research and starting organizations like Black in AI and the Distributed Artificial Intelligence Research Institute.\n\n\nTutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision\nIn Dr. Gebru’s talk, she speaks about how the field of computer vision is negatively affecting certain groups of people. She stated that computer vision is being utilized for things like the hiring process, surveillance, and policing. Although computer vision has its pros, there are also cons. For example, Dr. Gebru points out the use of face recognition in Maryland, where they used it to identify the Annapolis Capital Gazette shooter, and while this was a benefit, the police started to use it on photos to identify individuals at the Freddit Gray protests. In addition to surveillance, there are racial issues of computer vision. She noted that there were high disparity between black women and white men. Facial recognition was accurate when identifying white men but were much less accurate when identifying black women. Some companies believed that they did not have enough data to correctly identify certain groups of people such as people of the transgender community. As a result, they used videos of Transgender Youtubers without permission to enhance their data for facial/gender recognition.\nComputer vision algorithms can be useful, but it can cause a lot more harm than one might think.\n\nQuestion for Dr. Gebru\nWhat can companies do to prevent algorithmic biases in computer vision?\n\n\n\nSummary of Dr. Gebru’s Talk\nThe main focus on Dr. Gebru’s talk on Monday was about AGI (artificial general intelligence). In her talk, she briefly goes over the first wave of eugenics, movement that sought to improve the human population through sterilization, selective breeding, etc. and also the second wave of eugenics. Dr. Gebru argues that the current growth of AGI is correlated to the second wave of eugenics. She mentions that both the first and the second wave of eugenics (mostly the latter) are issues that are starting to surface as the field of AGI grows because these AGI models select the traits of humans that it deems helpful to the human population.\nDr. Gebru also explains that AGI is being used to create a utopian society. She describes that basic human needs will be met and the problems of the world such as poverty will be solved through the development in AGI. However, she argues that this utopian society that is promised through AGI will only benefit the rich. Another reason she links the development in AGI to second wave eugenics is due to its transhumanism ideology. Dr. Gebru notes that the field of AGI is being geared towards developing a society where humans and technology are integrated. Transhumanists believe that the integration of human and technology will improve society as a whole. Dr. Gebru raises concerns for this ideology and that only the privileged will be able to improve their living standards while others are left behind.\nI agree with the arguments of Dr. Gebru. Although I am fairly new to the ideas that Dr. Gebru has brought up in the talk, I believe I can agree with the fact that TESCREAL is ethically wrong even if they believe they are doing good for the future of the human population. AI may not seem like it is related to the second wave eugenics, but I can agree with Dr. Gebru with the fact that in the near future, their relation will become clearer.\n\n\nMy Thoughts\nAttending the talk by Dr. Gebru was a great experience and the topic was very interesting. Before attending the talk, I had never heard any of the TESCREAL terms, but after attending, it helped me aware of what is going on in the field of artificial intelligence and that it is being pushed in this direction. I became interested and researched on my own time about how the second wave of eugenics is related to AGI. Something that I was sort of aware was transhumanism; the integration of man and machine is something that I have always disagreed with and after the talk I became certain of my opinion."
  }
]