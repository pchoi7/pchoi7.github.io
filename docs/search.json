[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/TimnitGebru/index.html",
    "href": "posts/TimnitGebru/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "About Dr. Timnit Gebru\nDr. Timnit Gebru is a computer scientist who specializes machine learning and artificial intelligence. She cofounded a research organization alongside Rediet Abebe known as Black in AI, which seeks to speak out about the lack of diversity in the field of artificial intelligence. Many of her works address racial discrimination and gender bias. Dr. Gebru coauthored a paper wher research showed that facial recognition was less accurate in identifying people of color and women. This would mean that there would be discrimination against these groups. This led to another research by other computer scientists and found that in facial recognition, black women were 35% less likely to be recognized than white men, further proving Dr. Gebru’s argument. Dr. Gebru is a recognized voice in artificial intelligence because she continues to uncover the ethical issues of the field even if it negatively impacts her. For example, she and five others did an research and put their findings on a paper called “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” during her time at Google. Despite the opposition from the higer-ups, Gebru insisted that the paper be published, which led to her departure from the company. Regardless of these things, she continues to move forward with the effort the address social injustices in the field of AI through research and starting organizations like Black in AI and the Distributed Artificial Intelligence Research Institute.\n\n\nTutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision\nIn Dr. Gebru’s talk, she speaks about how the field of computer vision is negatively affecting certain groups of people. She stated that computer vision is being utilized for things like the hiring process, surveillance, and policing. Although computer vision has its pros, there are also cons. For example, Dr. Gebru points out the use of face recognition in Maryland, where they used it to identify the Annapolis Capital Gazette shooter, and while this was a benefit, the police started to use it on photos to identify individuals at the Freddit Gray protests. In addition to surveillance, there are racial issues of computer vision. She noted that there were high disparity between black women and white men. Facial recognition was accurate when identifying white men but were much less accurate when identifying black women. Some companies believed that they did not have enough data to correctly identify certain groups of people such as people of the transgender community. As a result, they used videos of Transgender Youtubers without permission to enhance their data for facial/gender recognition.\nComputer vision algorithms can be useful, but it can cause a lot more harm than one might think.\n\nQuestion for Dr. Gebru\nWhat can companies do to prevent algorithmic biases in computer vision?\n\n\n\nSummary of Dr. Gebru’s Talk\nThe main focus on Dr. Gebru’s talk on Monday was about AGI (artificial general intelligence). In her talk, she briefly goes over the first wave of eugenics, movement that sought to improve the human population through sterilization, selective breeding, etc. and also the second wave of eugenics. Dr. Gebru argues that the current growth of AGI is correlated to the second wave of eugenics. She mentions that both the first and the second wave of eugenics (mostly the latter) are issues that are starting to surface as the field of AGI grows because these AGI models select the traits of humans that it deems helpful to the human population.\nDr. Gebru also explains that AGI is being used to create a utopian society. She describes that basic human needs will be met and the problems of the world such as poverty will be solved through the development in AGI. However, she argues that this utopian society that is promised through AGI will only benefit the rich. Another reason she links the development in AGI to second wave eugenics is due to its transhumanism ideology. Dr. Gebru notes that the field of AGI is being geared towards developing a society where humans and technology are integrated. Transhumanists believe that the integration of human and technology will improve society as a whole. Dr. Gebru raises concerns for this ideology and that only the privileged will be able to improve their living standards while others are left behind.\nI agree with the arguments of Dr. Gebru. Although I am fairly new to the ideas that Dr. Gebru has brought up in the talk, I believe I can agree with the fact that TESCREAL is ethically wrong even if they believe they are doing good for the future of the human population. AI may not seem like it is related to the second wave eugenics, but I can agree with Dr. Gebru with the fact that in the near future, their relation will become clearer.\n\n\nMy Thoughts\nAttending the talk by Dr. Gebru was a great experience and the topic was very interesting. Before attending the talk, I had never heard any of the TESCREAL terms, but after attending, it helped me aware of what is going on in the field of artificial intelligence and that it is being pushed in this direction. I became interested and researched on my own time about how the second wave of eugenics is related to AGI. Something that I was sort of aware was transhumanism; the integration of man and machine is something that I have always disagreed with and after the talk I became certain of my opinion."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Logistic Regression\n\n\n\n\n\nBlog Post for Logistic Regression\n\n\n\n\n\n\nMay 13, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nTimnit Gebru\n\n\n\n\n\nBlog Post about Dr. Gebru’s Talk\n\n\n\n\n\n\nApr 20, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nPerceptron\n\n\n\n\n\nPerceptron Blog Post\n\n\n\n\n\n\nMar 10, 2023\n\n\nPaul Choi\n\n\n\n\n\n\n  \n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blogg"
  },
  {
    "objectID": "posts/Perceptron/perceptronNB.html",
    "href": "posts/Perceptron/perceptronNB.html",
    "title": "Perceptron",
    "section": "",
    "text": "Here is the link to the source code: https://github.com/pchoi7/pchoi7.github.io/blob/main/posts/Perceptron/perceptron.py\n\nfrom perceptron import Perceptron\n\n\nPerceptron Implementation\nIn order to perform the perceptron, I created a fit function. In this fit function, I initiate a feature matrix \\(\\tilde{X}\\) of 1’s. The weight vector is given a random number and a for loop is made to perform the perceptron update. In this for loop, the score is appended into the history and the weight is updated using the equation below:\n\\(\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_{i}\\langle \\tilde{w}^{(t)},\\tilde{x}_{i}\\rangle &lt; 0)\\tilde{y}_{i}\\tilde{x}_{i}\\)\n\n\nExperiment (Linearly Separable)\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nprint(\"The accuracy of the perceptron is \" + str(p.score(X, y)))\nprint(\"Score evolution over time is \" + str(p.history[-10:]))\n\nThe accuracy of the perceptron is 1.0\nScore evolution over time is [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\n\n\nExperiment (Non-linearly Separable)\nFor this part of the experiment, the graph was set up so that it would be impossible to be linearly separable. We can see that the data cannot be separated linearly because some of the purple points overlap the yellow data. The accuracy for this data set will never reach 100%.\n\nnp.random.seed(12345)\n\np_2 = Perceptron()\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-0.2, -0.2), (1.5, 1.5)])\n\nfig_2 = plt.scatter(X_2[:,0], X_2[:,1], c = y)\nfig_2 = draw_line(p.w, -2, 2)\n\nxlab_2 = plt.xlabel(\"Feature 1\")\nylab_2 = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig_2 = plt.plot(p_2.history)\nxlab_2 = plt.xlabel(\"Iteration\")\nylab_2 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np_2 = Perceptron()\np_2.fit(X_2, y_2, max_steps = 1000)\n\nprint(\"The accuracy of the perceptron is \" + str(p_2.score(X_2, y_2)))\nprint(\"Score evolution over time is \" + str(p_2.history[-10:]))\n\nThe accuracy of the perceptron is 0.88\nScore evolution over time is [0.84, 0.84, 0.56, 0.71, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88]\n\n\nWe can see that with this data set, the accuracy may fall well below 50-60% for a few iterations. Overall, the perceptron algorithm is pretty accurate (88%).\n\n\nExperiment (Multi-dimensional)\n\nnp.random.seed(12345)\n\np_3 = Perceptron()\n\np_features = 10\nX_3, y_3 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.8, -1.8), (1.8, 1.8)])\n\np_3.fit(X_3, y_3, max_steps = 1000)\n\nfig_3 = plt.plot(p_3.history)\nxlab_3 = plt.xlabel(\"Iteration\")\nylab_3 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nprint(\"The accuracy of the perceptron is \" + str(p_3.score(X_3, y_3)))\nprint(\"Score evolution over time is \" + str(p_3.history[-10:]))\n\nThe accuracy of the perceptron is 1.0\nScore evolution over time is [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nThis data set of 9 dimensions is linearly separable because it reaches 1.0 accuracy.\n\n\nRuntime Complexity\nFor a single iteration of the perceptron algorithm update, the runtime is O(p). The operations that affect the equation \\(\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_{i}\\langle \\tilde{w}^{(t)},\\tilde{x}_{i}\\rangle &lt; 0)\\tilde{y}_{i}\\tilde{x}_{i}\\) is the dot product, multiplication, then addition. Performing the dot product is O(p) + O(p), while performing multiplication and addition is O(1) for both. Therefore, the runtime for equation 1 is O(p)."
  },
  {
    "objectID": "posts/Logistic-Regression/LR.html",
    "href": "posts/Logistic-Regression/LR.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "URL to source code: code\n\nLogistic Regression Implementation\nIn order to implement the \\(\\textit{fit}\\) function, I used the equation for the gradient as follows:\n\\(\\nabla L(w) = \\frac{1}{n} \\sum_{i = 1}^{n}\\nabla \\ell (f_{w}(x_{i}), y_{i})\\)\nThis equation is the gradient for empirical risk for logistic regression. Using this equation, we can substitute it into \\(w^{(t +1)} \\leftarrow w^{(t)} - \\alpha \\nabla L(w^{(t)})\\) and choose our value of \\(\\alpha\\) until it converges.\nFor stochastic gradient descent, a similar equation to the \\(\\textit{fit}\\) function was used:\n\\(\\nabla_{S}L(w) = \\frac{1}{|S|} \\sum_{i \\in S}\\nabla \\ell (f_{w}(x_{i}), y_{i})\\)\nwhere \\(S \\subseteq [n] = \\{1,...,n\\}\\) and is called the batch size.\n\nfrom logReg import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nEvolution of Loss Function for Gradient Descent and Stochastic Gradient Descent\n\n# GRADIENT\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\n\n# STOCHASTIC GRADIENT\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\n\nExperiment 1 (Large Learning rate \\(\\alpha\\))\nIn this experiment, I changed the values of the learning rate \\(\\alpha\\) to see which values fails to converge.\n\nalpha_list = [0.1, 10, 100]  # list of learning rates to test\n\nfor i in range(len(alpha_list)):\n    \n    if i == 0:\n        \n        LR = LogisticRegression()\n        LR.fit(X, y, alpha = alpha_list[i], max_epochs = 75)\n    \n        loss = len(LR.loss_history)\n        plt.plot(np.arange(loss) + 1, LR.loss_history, label = r\"$\\alpha$ = 0.1\")\n    \n    else:\n           \n        LR = LogisticRegression()\n        LR.fit(X, y, alpha = alpha_list[i], max_epochs = 75)\n    \n        loss = len(LR.loss_history)\n        plt.plot(np.arange(loss) + 1, LR.loss_history, label = r\"$\\alpha$ = %d\"%alpha_list[i])\n    \n\nplt.legend()\nplt.xlabel('Number of iterations')\nplt.ylabel('Loss')\nplt.show()\n\n\n\n\nThe loss of a learning rate too high will not converge. In this case, I’ve used a learning rate of \\(\\alpha = 100\\), and the graph shows that for this value the loss does not converge.\n\n\nExperiment 2 (Changing Batch Size)\nIn this experiment, I am changing the batch size to see how it influences the convergence of the algorithm. This experiment uses the fit_stochastic function.\n\n#LR = LogisticRegression()\n\nbatch_list = [10, 50, 100, 1000]  # list of batch sizes to test\n\nfor i in range(len(batch_list)):\n    \n    LR = LogisticRegression()\n    LR.fit_stochastic(X, y, alpha = 0.1, batch_size = batch_list[i], max_epochs = 100)\n    \n    loss = len(LR.loss_history)\n    plt.plot(np.arange(loss) + 1, LR.loss_history, label = \"batch size = %d\"%batch_list[i])\n    \n    \nplt.legend()\nplt.xlabel('Number of iterations')\nplt.ylabel('Loss')\nplt.show()\n\n\n\n\nAs shown in this graph, when the batch size is smaller, the algorithm converges faster."
  }
]